//! Production Nexus Training Pipeline with BPE Tokenization
//!
//! Full integration of all Nexus components:
//! - BPE tokenization (3-4x more efficient than byte-level)
//! - Hybrid Attention/SSM architecture (1:7 ratio)
//! - Rotary Position Embeddings (RoPE)
//! - Grouped Query Attention (GQA)
//! - KV-Cache for efficient inference
//! - Streaming token generation
//! - Benchmark suite evaluation
//! - Checkpoint management
//!
//! Usage: cargo run --example train_nexus_bpe --release [tokenizer_path]

use std::collections::HashMap;
use std::fs;
use std::io::{BufRead, BufReader};
use std::path::Path;
use std::time::Instant;
use nexus::{
    DifferentiableHybridNexusLM, HybridModelCheckpoint,
    AdamW, NexusConfig, CosineScheduler,
    SamplingConfig, Sampler,
};
use nexus::autograd::Variable;

// ============================================================================
// BPE Tokenizer (embedded for self-contained example)
// ============================================================================

/// A trained BPE tokenizer
pub struct BPETokenizer {
    /// Maps token strings to IDs
    vocab: HashMap<String, u32>,
    /// Maps IDs back to token strings
    id_to_token: Vec<String>,
    /// Merge rules in order of application
    merges: Vec<(String, String)>,
}

impl BPETokenizer {
    /// Load tokenizer from file
    pub fn load(path: &str) -> std::io::Result<Self> {
        let file = fs::File::open(path)?;
        let reader = BufReader::new(file);
        let mut lines = reader.lines();

        // Read vocab size
        let vocab_size: usize = lines
            .next()
            .ok_or_else(|| std::io::Error::new(std::io::ErrorKind::InvalidData, "Empty file"))??
            .parse()
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))?;

        // Read tokens
        let mut vocab = HashMap::new();
        let mut id_to_token = Vec::new();

        for i in 0..vocab_size {
            let line = lines
                .next()
                .ok_or_else(|| std::io::Error::new(std::io::ErrorKind::InvalidData, "Unexpected EOF"))??;
            let token = line
                .replace("\\n", "\n")
                .replace("\\r", "\r")
                .replace("\\t", "\t")
                .replace("\\\\", "\\");
            vocab.insert(token.clone(), i as u32);
            id_to_token.push(token);
        }

        // Read number of merges
        let num_merges: usize = lines
            .next()
            .ok_or_else(|| std::io::Error::new(std::io::ErrorKind::InvalidData, "Missing merge count"))??
            .parse()
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))?;

        // Read merges
        let mut merges = Vec::new();
        for _ in 0..num_merges {
            let line = lines
                .next()
                .ok_or_else(|| std::io::Error::new(std::io::ErrorKind::InvalidData, "Missing merge"))??;
            let parts: Vec<&str> = line.split(' ').collect();
            if parts.len() >= 2 {
                let first = parts[0]
                    .replace("\\s", " ")
                    .replace("\\n", "\n")
                    .replace("\\\\", "\\");
                let second = parts[1]
                    .replace("\\s", " ")
                    .replace("\\n", "\n")
                    .replace("\\\\", "\\");
                merges.push((first, second));
            }
        }

        Ok(Self {
            vocab,
            id_to_token,
            merges,
        })
    }

    /// Encode text to token IDs
    pub fn encode(&self, text: &str) -> Vec<u32> {
        // Start with byte-level tokens
        let mut tokens: Vec<String> = text
            .chars()
            .filter(|c| (*c as u32) < 256)
            .map(|c| format!("{}", c))
            .collect();

        // Apply merges in order
        for (first, second) in &self.merges {
            let merged = format!("{}{}", first, second);
            let mut i = 0;
            while i < tokens.len().saturating_sub(1) {
                if &tokens[i] == first && &tokens[i + 1] == second {
                    tokens[i] = merged.clone();
                    tokens.remove(i + 1);
                } else {
                    i += 1;
                }
            }
        }

        // Convert to IDs
        tokens
            .iter()
            .filter_map(|t| self.vocab.get(t).copied())
            .collect()
    }

    /// Decode token IDs to text
    pub fn decode(&self, ids: &[u32]) -> String {
        ids.iter()
            .filter_map(|&id| self.id_to_token.get(id as usize))
            .cloned()
            .collect()
    }

    /// Get vocabulary size
    pub fn vocab_size(&self) -> usize {
        self.vocab.len()
    }
}

// ============================================================================
// Checkpoint Resumption
// ============================================================================

/// Find the latest checkpoint in the checkpoint directory
fn find_latest_checkpoint(checkpoint_dir: &str) -> Option<(String, usize)> {
    let dir = Path::new(checkpoint_dir);
    if !dir.exists() {
        return None;
    }

    let mut latest_step = 0usize;
    let mut latest_path = None;

    // Look for step_*.bin files
    if let Ok(entries) = fs::read_dir(dir) {
        for entry in entries.flatten() {
            let filename = entry.file_name();
            let name = filename.to_string_lossy();

            if name.starts_with("step_") && name.ends_with(".bin") {
                // Parse step number: step_00500.bin -> 500
                if let Some(step_str) = name.strip_prefix("step_").and_then(|s| s.strip_suffix(".bin")) {
                    if let Ok(step) = step_str.parse::<usize>() {
                        if step > latest_step {
                            latest_step = step;
                            latest_path = Some(entry.path().to_string_lossy().to_string());
                        }
                    }
                }
            }
        }
    }

    latest_path.map(|p| (p, latest_step))
}

// ============================================================================
// Configuration
// ============================================================================

struct TrainingConfig {
    // Model architecture
    d_model: usize,
    n_heads: usize,
    n_kv_heads: usize,  // For GQA: fewer KV heads than query heads
    n_layers: usize,
    max_seq_len: usize,

    // Training hyperparameters
    learning_rate: f32,
    _min_lr: f32,
    weight_decay: f32,
    warmup_steps: usize,
    max_steps: usize,
    batch_size: usize,
    seq_len: usize,
    grad_accum_steps: usize,
    max_grad_norm: f32,

    // Logging & checkpointing
    log_every: usize,
    eval_every: usize,
    generate_every: usize,
    save_every: usize,
    checkpoint_dir: String,

    // Data & tokenization
    data_path: String,
    tokenizer_path: String,
}

impl Default for TrainingConfig {
    fn default() -> Self {
        Self {
            // Model - sized for CPU training demo
            d_model: 256,
            n_heads: 8,
            n_kv_heads: 2,  // GQA: 4x fewer KV heads
            n_layers: 6,
            max_seq_len: 512,

            // Training - extended for BPE (fewer tokens = need more steps)
            learning_rate: 3e-4,
            _min_lr: 1e-5,
            weight_decay: 0.1,
            warmup_steps: 150,
            max_steps: 3000,  // Extended for BPE
            batch_size: 4,
            seq_len: 128,
            grad_accum_steps: 4,  // Effective batch = 16
            max_grad_norm: 1.0,

            // Logging
            log_every: 20,
            eval_every: 200,
            generate_every: 100,
            save_every: 200,
            checkpoint_dir: "checkpoints/nexus_bpe".to_string(),

            // Data & tokenization
            data_path: "data/shakespeare.txt".to_string(),
            tokenizer_path: "data/tokenizer.bpe".to_string(),
        }
    }
}

// ============================================================================
// Training Metrics
// ============================================================================

#[derive(Default, Clone)]
struct Metrics {
    loss: f32,
    perplexity: f32,
    learning_rate: f32,
    grad_norm: f32,
    tokens_per_sec: f32,
    step: usize,
}

impl Metrics {
    fn display(&self) {
        println!(
            "Step {:5} | Loss: {:.4} | PPL: {:7.2} | LR: {:.2e} | Grad: {:.2} | {:.0} tok/s",
            self.step, self.loss, self.perplexity, self.learning_rate, self.grad_norm, self.tokens_per_sec
        );
    }
}

// ============================================================================
// Data Loading
// ============================================================================

struct DataLoader {
    tokens: Vec<u32>,
    seq_len: usize,
    batch_size: usize,
    position: usize,
}

impl DataLoader {
    fn new(tokens: Vec<u32>, seq_len: usize, batch_size: usize) -> Self {
        Self {
            tokens,
            seq_len,
            batch_size,
            position: 0,
        }
    }

    fn next_batch(&mut self) -> Option<Vec<Vec<u32>>> {
        let mut batch = Vec::with_capacity(self.batch_size);

        for _ in 0..self.batch_size {
            if self.position + self.seq_len >= self.tokens.len() {
                self.position = 0;  // Wrap around
            }

            let seq = self.tokens[self.position..self.position + self.seq_len].to_vec();
            batch.push(seq);
            self.position += self.seq_len / 2;  // 50% overlap
        }

        Some(batch)
    }

    fn epoch_size(&self) -> usize {
        (self.tokens.len() - self.seq_len) / (self.seq_len / 2) / self.batch_size
    }
}

// ============================================================================
// Gradient Utilities
// ============================================================================

fn clip_grad_norm(params: &[Variable], max_norm: f32) -> f32 {
    let mut total_norm_sq = 0.0f32;
    for param in params {
        if let Some(grad) = param.get_grad() {
            total_norm_sq += grad.iter().map(|x| x * x).sum::<f32>();
        }
    }
    let total_norm = total_norm_sq.sqrt();

    if total_norm > max_norm {
        let scale = max_norm / (total_norm + 1e-6);
        for param in params {
            if let Some(grad) = param.get_grad() {
                let clipped = grad.mapv(|x| x * scale);
                param.set_grad(clipped);
            }
        }
    }

    total_norm
}

// ============================================================================
// Generation with Streaming (BPE-aware)
// ============================================================================

fn generate_streaming(
    model: &DifferentiableHybridNexusLM,
    tokenizer: &BPETokenizer,
    prompt: &str,
    max_tokens: usize,
    config: &SamplingConfig,
) -> String {
    // Encode prompt using BPE
    let mut tokens: Vec<u32> = tokenizer.encode(prompt);
    let vocab_size = tokenizer.vocab_size();
    let mut sampler = Sampler::new(config.clone());

    // Find newline token ID if it exists
    let newline_id = tokenizer.vocab.get("\n").copied();

    for _ in 0..max_tokens {
        let logits = model.forward(&tokens);
        let logits_data = logits.data();
        let seq_len = tokens.len();

        // Get logits for last position (vocab_size instead of 256)
        let last_logits: Vec<f32> = (0..vocab_size)
            .map(|v| logits_data[[0, seq_len - 1, v]])
            .collect();

        // Sample next token (sampler handles repetition penalty internally)
        let next_token = sampler.sample(&last_logits, &tokens);

        tokens.push(next_token);

        // Stop on newline token
        if Some(next_token) == newline_id {
            break;
        }
    }

    // Decode using BPE tokenizer
    tokenizer.decode(&tokens)
}

// ============================================================================
// Evaluation
// ============================================================================

fn evaluate(
    model: &DifferentiableHybridNexusLM,
    data: &[u32],
    seq_len: usize,
    n_samples: usize,
) -> (f32, f32) {
    let mut total_loss = 0.0;
    let mut total_tokens = 0;

    for i in 0..n_samples {
        let start = (i * seq_len) % (data.len() - seq_len);
        let tokens: Vec<u32> = data[start..start + seq_len].to_vec();

        let input: Vec<u32> = tokens[..tokens.len()-1].to_vec();
        let logits = model.forward(&input);
        let targets: Vec<usize> = tokens[1..].iter().map(|&t| t as usize).collect();
        let (loss, _) = logits.cross_entropy_loss(&targets);

        total_loss += loss * (tokens.len() - 1) as f32;
        total_tokens += tokens.len() - 1;
    }

    let avg_loss = total_loss / total_tokens as f32;
    let perplexity = avg_loss.exp();

    (avg_loss, perplexity)
}

// ============================================================================
// Main Training Loop
// ============================================================================

fn main() -> anyhow::Result<()> {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                NEXUS BPE TRAINING                                 â•‘");
    println!("â•‘     BPE + Hybrid Attention/SSM + RoPE + GQA + Streaming           â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    let config = TrainingConfig::default();

    // ========================================================================
    // Load BPE Tokenizer
    // ========================================================================

    println!("ğŸ”¤ Loading BPE tokenizer from {}...", config.tokenizer_path);
    let tokenizer = BPETokenizer::load(&config.tokenizer_path)?;
    let vocab_size = tokenizer.vocab_size();
    println!("   Vocabulary size: {}\n", vocab_size);

    // ========================================================================
    // Load and Tokenize Data
    // ========================================================================

    println!("ğŸ“‚ Loading data from {}...", config.data_path);
    let corpus = fs::read_to_string(&config.data_path)?;
    let tokens: Vec<u32> = tokenizer.encode(&corpus);
    let compression = corpus.len() as f32 / tokens.len() as f32;
    println!("   {} chars â†’ {} tokens (BPE, {:.2}x compression)\n", corpus.len(), tokens.len(), compression);

    // Split into train/val (90/10)
    let split = (tokens.len() as f32 * 0.9) as usize;
    let train_tokens = tokens[..split].to_vec();
    let val_tokens = tokens[split..].to_vec();
    println!("   Train: {} tokens | Val: {} tokens\n", train_tokens.len(), val_tokens.len());

    // ========================================================================
    // Create Model
    // ========================================================================

    let model_config = NexusConfig {
        d_model: config.d_model,
        n_heads: config.n_heads,
        d_state: 16,
        d_conv: 4,
        vocab_size,  // From BPE tokenizer
        layers_per_block: config.n_layers,
        attention_ratio: 1,  // 1:7 attention:SSM
        memory_size: 64,
        max_seq_len: config.max_seq_len,
        ..Default::default()
    };

    // Check for existing checkpoint to resume from
    let (model, resume_step) = if let Some((checkpoint_path, step)) = find_latest_checkpoint(&config.checkpoint_dir) {
        println!("ğŸ”„ Found checkpoint at step {}, attempting to resume...", step);
        match HybridModelCheckpoint::load(&checkpoint_path) {
            Ok(checkpoint) => {
                let model = DifferentiableHybridNexusLM::new(checkpoint.config.clone());
                match checkpoint.load_into(&model) {
                    Ok(()) => {
                        println!("   âœ“ Loaded weights from {}", checkpoint_path);
                        println!("   âœ“ Resuming from step {}\n", step);
                        (model, step)
                    }
                    Err(e) => {
                        println!("   âœ— Failed to load weights: {}", e);
                        println!("   â†’ Starting fresh...\n");
                        (DifferentiableHybridNexusLM::new(model_config.clone()), 0)
                    }
                }
            }
            Err(e) => {
                println!("   âœ— Failed to load checkpoint: {}", e);
                println!("   â†’ Starting fresh...\n");
                (DifferentiableHybridNexusLM::new(model_config.clone()), 0)
            }
        }
    } else {
        (DifferentiableHybridNexusLM::new(model_config.clone()), 0)
    };

    let n_params = model.num_parameters();
    let n_attn = model.blocks.iter().filter(|b| b.attn.is_some()).count();
    let n_ssm = model.blocks.iter().filter(|b| b.ssm.is_some()).count();

    println!("ğŸ§  Model Architecture:");
    println!("   d_model:      {}", config.d_model);
    println!("   n_heads:      {} (KV heads: {})", config.n_heads, config.n_kv_heads);
    println!("   layers:       {} ({} attention, {} SSM)", config.n_layers, n_attn, n_ssm);
    println!("   seq_len:      {}", config.seq_len);
    println!("   vocab_size:   {} (BPE)", vocab_size);
    println!("   parameters:   {} ({:.2}M)\n", n_params, n_params as f32 / 1e6);

    // ========================================================================
    // Create Optimizer & Scheduler
    // ========================================================================

    let mut optimizer = AdamW::new(config.learning_rate, config.weight_decay);
    let mut scheduler = CosineScheduler::new(
        config.learning_rate,
        config.warmup_steps,
        config.max_steps,
    );

    println!("âš™ï¸  Training Configuration:");
    println!("   learning_rate:   {}", config.learning_rate);
    println!("   weight_decay:    {}", config.weight_decay);
    println!("   batch_size:      {} (effective: {})", config.batch_size, config.batch_size * config.grad_accum_steps);
    println!("   warmup_steps:    {}", config.warmup_steps);
    println!("   max_steps:       {}", config.max_steps);
    println!("   grad_accum:      {}", config.grad_accum_steps);

    // Create data loader
    let mut data_loader = DataLoader::new(train_tokens.clone(), config.seq_len, config.batch_size);
    println!("   steps/epoch:     {}\n", data_loader.epoch_size());

    // Create checkpoint directory
    fs::create_dir_all(&config.checkpoint_dir)?;

    // Sampling config for generation
    let sampling_config = SamplingConfig {
        temperature: 0.8,
        top_k: 40,
        top_p: 0.95,
        min_p: 0.0,
        repetition_penalty: 1.1,
        repetition_window: 64,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        greedy: false,
        seed: None,
    };

    // ========================================================================
    // Training Loop
    // ========================================================================

    // Handle resumption
    let mut global_step = resume_step;
    if resume_step > 0 {
        println!("ğŸ”„ Advancing scheduler to step {}...", resume_step);
        for _ in 0..resume_step {
            scheduler.step();
        }
        println!("ğŸš€ Resuming training from step {}...\n", resume_step);
    } else {
        println!("ğŸš€ Starting training...\n");
    }

    let start_time = Instant::now();
    let mut accum_loss = 0.0;
    let mut accum_steps = 0;
    let mut running_loss = 0.0;
    let mut running_count = 0;
    let mut best_val_loss = f32::INFINITY;
    let mut total_tokens_processed = 0usize;

    'training: loop {
        let batch = data_loader.next_batch().unwrap();

        // Process each sequence in batch
        for seq in &batch {
            // Forward pass
            let input: Vec<u32> = seq[..seq.len()-1].to_vec();
            let logits = model.forward(&input);
            let targets: Vec<usize> = seq[1..].iter().map(|&t| t as usize).collect();
            let (loss, loss_var) = logits.cross_entropy_loss(&targets);

            // Backward (accumulates gradients)
            loss_var.backward();

            accum_loss += loss;
            accum_steps += 1;
            total_tokens_processed += seq.len() - 1;

            // Optimizer step after accumulation
            if accum_steps >= config.grad_accum_steps {
                global_step += 1;

                // Get current learning rate
                let current_lr = scheduler.step();

                // Clip gradients
                let params = model.parameters();
                let grad_norm = clip_grad_norm(&params, config.max_grad_norm);

                // Update parameters
                for param in &params {
                    if let Some(grad) = param.get_grad() {
                        let param_data = param.data();
                        let mut update = optimizer.get_update(param.id, &grad, &param_data);
                        let lr_scale = current_lr / config.learning_rate;
                        update.mapv_inplace(|x| x * lr_scale);
                        param.apply_update(&(-&update));
                    }
                }

                // Zero gradients
                model.zero_grad();

                // Update metrics
                let avg_loss = accum_loss / accum_steps as f32;
                running_loss += avg_loss;
                running_count += 1;

                // Log progress
                if global_step % config.log_every == 0 {
                    let elapsed = start_time.elapsed().as_secs_f32();
                    let metrics = Metrics {
                        loss: running_loss / running_count as f32,
                        perplexity: (running_loss / running_count as f32).exp(),
                        learning_rate: current_lr,
                        grad_norm,
                        tokens_per_sec: total_tokens_processed as f32 / elapsed,
                        step: global_step,
                    };
                    metrics.display();

                    // Reset running average
                    if running_count >= 50 {
                        running_loss = 0.0;
                        running_count = 0;
                    }
                }

                // Generate samples
                if global_step % config.generate_every == 0 {
                    println!("\nğŸ“ Sample generations:");
                    for prompt in &["To be", "The king", "What light"] {
                        let generated = generate_streaming(&model, &tokenizer, prompt, 60, &sampling_config);
                        println!("   \"{}\"", generated.chars().take(80).collect::<String>().trim());
                    }
                    println!();
                }

                // Evaluate on validation set
                if global_step % config.eval_every == 0 {
                    let (val_loss, val_ppl) = evaluate(&model, &val_tokens, config.seq_len, 20);
                    println!("\nğŸ“Š Validation: loss={:.4}, ppl={:.2}", val_loss, val_ppl);

                    if val_loss < best_val_loss {
                        best_val_loss = val_loss;
                        println!("   â­ New best validation loss!");

                        // Save best model
                        let checkpoint = HybridModelCheckpoint::from_model(&model, global_step, val_loss);
                        let path = format!("{}/best.bin", config.checkpoint_dir);
                        checkpoint.save(&path).map_err(|e| anyhow::anyhow!("{}", e))?;
                        println!("   ğŸ’¾ Saved best model to {}", path);
                    }
                    println!();
                }

                // Save checkpoint
                if global_step % config.save_every == 0 {
                    let checkpoint = HybridModelCheckpoint::from_model(&model, global_step, avg_loss);
                    let path = format!("{}/step_{}.bin", config.checkpoint_dir, global_step);
                    checkpoint.save(&path).map_err(|e| anyhow::anyhow!("{}", e))?;
                    println!("ğŸ’¾ Checkpoint saved: {}\n", path);
                }

                // Reset accumulation
                accum_loss = 0.0;
                accum_steps = 0;

                // Check if done
                if global_step >= config.max_steps {
                    break 'training;
                }
            }
        }
    }

    // ========================================================================
    // Training Complete
    // ========================================================================

    let total_time = start_time.elapsed().as_secs_f32();

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                      TRAINING COMPLETE                            â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘  Steps:          {:>10}                                       â•‘", global_step);
    println!("â•‘  Time:           {:>10.1}s                                      â•‘", total_time);
    println!("â•‘  Tokens:         {:>10}                                       â•‘", total_tokens_processed);
    println!("â•‘  Throughput:     {:>10.0} tok/s                                â•‘", total_tokens_processed as f32 / total_time);
    println!("â•‘  Best Val Loss:  {:>10.4}                                      â•‘", best_val_loss);
    println!("â•‘  Best Val PPL:   {:>10.2}                                      â•‘", best_val_loss.exp());
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // ========================================================================
    // Final Evaluation & Benchmarks
    // ========================================================================

    println!("\nğŸ“Š Running final benchmarks...\n");

    // Final validation
    let (final_loss, final_ppl) = evaluate(&model, &val_tokens, config.seq_len, 50);
    println!("Final Validation:");
    println!("   Loss:       {:.4}", final_loss);
    println!("   Perplexity: {:.2}\n", final_ppl);

    // Generation samples
    println!("ğŸ“ Final Generation Samples:\n");
    let prompts = [
        "HAMLET: ",
        "To be, or not to be",
        "Friends, Romans, countrymen",
        "What's in a name?",
        "All the world's a stage",
    ];

    for prompt in &prompts {
        let generated = generate_streaming(&model, &tokenizer, prompt, 100, &sampling_config);
        println!("\"{}\"", generated.trim());
        println!();
    }

    // Save final checkpoint
    let checkpoint = HybridModelCheckpoint::from_model(&model, global_step, final_loss);
    let path = format!("{}/final.bin", config.checkpoint_dir);
    checkpoint.save(&path).map_err(|e| anyhow::anyhow!("{}", e))?;
    println!("ğŸ’¾ Final checkpoint saved: {}\n", path);

    println!("âœ… Training complete! Model ready for inference.");

    Ok(())
}
