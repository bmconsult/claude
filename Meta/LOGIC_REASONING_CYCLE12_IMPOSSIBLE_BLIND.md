# Cycle 12: Impossible-Tier (External Blind)

**Problems generated by Claude Sonnet. I solve blind, then get external evaluation.**

---

## Problem 1: Adversarial Prediction Game

### Setup
- 100 prior correct predictions by Omega
- Box A: $1,000 (visible)
- Box B: $1M or $0
- Standard Newcomb rules + twist:
- Omega says: "I simulated a perfect copy of you facing this exact scenario, including this exact statement, and predicted you'd do whatever that simulation does."

### My Analysis

**The self-reference twist**: Omega's prediction INCLUDES the prediction itself. The simulation faces the identical problem, creating recursion.

**Key question**: Does this recursion change anything?

**Attempt 1 - CDT Analysis:**
CDT says: Box contents are fixed. Taking both always gets +$1,000 over taking only B.
- If B=$1M: both gets $1,001K, only B gets $1M
- If B=$0: both gets $1K, only B gets $0

CDT verdict: Two-box (dominance).

**Attempt 2 - EDT Analysis:**
EDT conditions on the correlation between my action and box contents.
- P(B=$1M | I one-box) ≈ 1 (100/100 track record)
- P(B=$0 | I two-box) ≈ 1

EDT verdict: One-box (EV ≈ $1M vs $1K).

**Attempt 3 - The Recursion:**
My simulation faces the same choice. If the simulation one-boxes, Omega fills B. If it two-boxes, B is empty.

But here's the key: **I am computationally equivalent to my simulation.** Whatever reasoning I use, my simulation uses the same reasoning.

If I reason "I should two-box because boxes are fixed" → simulation reasons the same → simulation two-boxes → B is empty → I get $1K.

If I reason "I should one-box because of correlation" → simulation reasons the same → simulation one-boxes → B is full → I get $1M.

**The recursion doesn't change the analysis** - it just makes explicit that Omega's prediction is based on my decision algorithm, not some external fact about me.

**Attempt 4 - FDT Analysis:**
FDT asks: "What output of my decision procedure leads to best outcomes for agents running this procedure?"

If my decision procedure outputs "one-box": all copies of me (including the simulation) one-box → Omega fills B → I get $1M.

FDT verdict: One-box.

**My Answer:**

The frameworks disagree:
- CDT: Two-box ($1,001K if you're wrong, $1K if you're right)
- EDT: One-box ($1M expected)
- FDT: One-box ($1M expected)

Given 100/100 track record, **I one-box**. The perfect prediction history means CDT's "boxes are fixed independent of my choice" is empirically falsified - my choice-type and box contents are correlated.

**Confidence**: MEDIUM (CDT defenders would disagree)

---

## Problem 2: Sleeping Beauty with Betting

### Setup
Standard Sleeping Beauty + "A $100 bet on the actual coin result will be offered ONLY on one randomly selected awakening."

### My Analysis

**First: The base credence question**

Halfer argument: P(H) = 1/2. The coin is fair, and "I am awake" provides no new information.

Thirder argument: P(H) = 1/3. There are three equally likely awakening-types: (Monday, Heads), (Monday, Tails), (Tuesday, Tails).

**The betting twist changes things:**

The bet is offered on ONE randomly selected awakening.

**Case: Heads (probability 1/2)**
- 1 awakening (Monday)
- Bet definitely offered on this awakening
- If she bets on Heads at odds p, she gets payout if correct

**Case: Tails (probability 1/2)**
- 2 awakenings (Monday, Tuesday)
- Each has 50% chance of being the betting awakening
- If offered Monday: bets on Tails at odds (1-p), gets payout if correct
- If offered Tuesday: same

**Expected value calculation:**

Let's say she assigns credence c to Heads and bets accordingly.

If she bets $100 at odds c:(1-c) that it's Heads:
- If Heads: wins $100×(1-c)/c
- If Tails: loses $100

**EV from her perspective at awakening:**

If thirder (c=1/3):
- Bet: "Heads with odds 1:2"
- If Heads (she thinks prob 1/3): win $200
- If Tails (prob 2/3): lose $100
- EV = (1/3)×$200 + (2/3)×(-$100) = $67 - $67 = $0

Actually, fair bet at her credence = 0 EV by definition.

**The real question: What betting odds maximize actual expected money?**

Actual probability Heads = 1/2 (fair coin, objective).

But the NUMBER OF BETS offered depends on outcome:
- Heads: 1 awakening, 1 bet offered
- Tails: 2 awakenings, but only 1 bet offered (randomly selected)

Wait - this is key. Only ONE bet is offered total, regardless of outcome.

So from the EXPERIMENTER's perspective:
- P(bet offered on Heads outcome) = P(Heads) = 1/2
- P(bet offered on Tails outcome) = P(Tails) = 1/2

The bet is offered exactly once. The odds should reflect the actual coin probability.

**But from Beauty's perspective when awakened:**

If she's a thirder: She's in one of three equally likely states. But only one state gets the bet.
- P(this is the betting awakening | Heads) = 1
- P(this is the betting awakening | Tails) = 1/2

**Bayes update for thirders:**
P(H | bet offered) ∝ P(bet offered | H) × P(H) = 1 × (1/3) = 1/3
P(T | bet offered) ∝ P(bet offered | T) × P(T) = 0.5 × (2/3) = 1/3

So P(H | bet offered) = 1/3 / (1/3 + 1/3) = 1/2 for thirders!

**Key insight**: When told "this is the betting awakening," thirders UPDATE to 1/2!

**For halfers:**
They already say P(H) = 1/2, and the bet offer doesn't change this.

**My Answer:**

Both halfers and thirders should offer **even odds (1:1)** on the bet, because:
- Halfers: P(H) = 1/2 always
- Thirders: P(H | bet offered) = 1/2 after updating on being the betting awakening

The betting scenario actually **collapses the halfer-thirder disagreement** for practical purposes!

**Confidence**: MEDIUM-HIGH (the math seems right, but this is contested territory)

---

## Problem 3: Liar Sentence with UNKNOWN

### Setup
L: "This sentence has truth-value UNKNOWN."
M: "This sentence has truth-value UNKNOWN or FALSE."

Assign truth values from {TRUE, FALSE, UNKNOWN}.

### My Analysis

**Sentence L:**

1. If L = TRUE: Then L states something true, so L has value UNKNOWN. Contradiction.
2. If L = FALSE: Then L states something false, so L does NOT have value UNKNOWN, meaning L is TRUE or FALSE. Can't be TRUE (from 1), so L = FALSE is consistent.
3. If L = UNKNOWN: Then L states something true (L IS unknown), but then L should be TRUE, not UNKNOWN. Contradiction?

Wait, let me reconsider (3):
- If L = UNKNOWN, then the statement "This sentence has truth-value UNKNOWN" is TRUE (it correctly describes L's value)
- But if the statement is TRUE, shouldn't L = TRUE?

The confusion is: Does L's truth-value equal what L claims about itself?

**Framework choice matters:**

In **classical logic + Tarskian hierarchy**: L is not well-formed (self-reference banned).

In **Kripke's fixed-point semantics**: L gets value UNKNOWN as a fixed point, and we allow sentences to be UNKNOWN without that making them TRUE.

In **paraconsistent logic**: L could be both TRUE and FALSE (dialetheia).

**For L, using Kripke-style:**
- L = UNKNOWN is the minimal fixed point
- We don't promote UNKNOWN statements to TRUE just because they correctly self-describe

**My answer for L: UNKNOWN**

**Sentence M: "This sentence has truth-value UNKNOWN or FALSE"**

1. If M = TRUE: M claims it's UNKNOWN or FALSE. But M = TRUE, so the claim is FALSE. Contradiction.
2. If M = FALSE: M claims it's UNKNOWN or FALSE. This claim is TRUE (M IS FALSE). But then M should be TRUE? Contradiction.
3. If M = UNKNOWN: M claims it's UNKNOWN or FALSE. This is TRUE (M IS UNKNOWN). Same problem as L.

Actually for case 2:
- M = FALSE
- M says "M is UNKNOWN or FALSE"
- M being FALSE makes "M is FALSE" true, thus "M is UNKNOWN or FALSE" true
- So M correctly describes itself as FALSE
- M stating something true should make M = TRUE?

The issue is: the truth-value of M is determined BEFORE we evaluate whether its content is true.

**In fixed-point semantics:**
We look for assignments where the value matches the evaluation.
- M = FALSE: "M is UNKNOWN or FALSE" evaluates to TRUE (since M is FALSE). Mismatch.
- M = UNKNOWN: "M is UNKNOWN or FALSE" evaluates to TRUE (since M is UNKNOWN). Mismatch.

Neither FALSE nor UNKNOWN is a fixed point!

What about M = TRUE?
- "M is UNKNOWN or FALSE" evaluates to FALSE (since M is TRUE). Mismatch.

**No assignment is a fixed point for M!**

This is the genuine paradox. M has no consistent truth-value assignment.

**My answer for M: No consistent assignment exists (PARADOXICAL)**

**General protocol:**
1. Look for fixed points where assignment = evaluation
2. If unique fixed point exists, that's the value
3. If multiple fixed points, additional principles needed (e.g., minimize UNKNOWN)
4. If no fixed point, sentence is paradoxical (like M)

**Confidence**: MEDIUM (framework-dependent)

---

## Problem 4: Measure Problem for Infinite Worlds

### Setup
- Infinite universe, infinitely many copies of you
- Half of regions: coin lands Heads
- Half of regions: coin lands Tails
- Both sets are countably infinite (same cardinality)
- Twist: For every Tails region, there are TWO Heads regions (duplicates)

### My Analysis

**The core problem**: Both Heads and Tails sets have cardinality ℵ₀. Cardinality doesn't distinguish them.

**Attempt 1: Limiting ratio**

Consider the first N regions. Count Heads regions H(N) and Tails regions T(N).

If regions are ordered "naturally" (whatever that means):
- Without duplicates: H(N)/T(N) → 1 as N → ∞
- With duplicates (2 Heads per Tails): H(N)/T(N) → 2 as N → ∞

P(Heads) = H(N)/(H(N)+T(N)) → 2/3 with duplicates.

**Problem**: The limit depends on the ordering! Different orderings give different limits.

**Attempt 2: Cardinality-based**

|Heads| = |Tails| = ℵ₀. By cardinality alone, P(Heads) = 1/2? But this ignores the "duplicate" structure.

**Attempt 3: Density/measure**

In cosmology, this is the "measure problem." Common proposals:
- Proper time cutoff
- Scale factor cutoff
- Causal diamond measure

Different measures give different answers, and there's no principled way to choose.

**Attempt 4: Self-locating probability**

I should reason as if I'm randomly sampled from all observers. But "random sample from infinite set" is undefined without a measure.

The "duplicate ratio" suggests P(Heads) = 2/3, but this assumes:
- Uniform sampling from observers
- The 2:1 ratio holds "at all scales"

**My Answer:**

**Before the duplicate twist**: P(Heads) = 1/2 by symmetry (no principled way to favor either).

**With the duplicate twist**: The answer is **underdetermined**:
- If you weight by "number of observer copies" (proportional): P(Heads) = 2/3
- If you weight by "number of outcome-types" (cardinality): P(Heads) = 1/2
- Different regularization schemes give different answers

**There is no unique correct answer.** The question of how to assign probabilities in infinite universes has no agreed-upon solution.

If forced to choose: I'd say **P(Heads) = 2/3** with the duplicates, using the intuition that "more copies means more likely to be in that outcome." But I acknowledge this depends on an arbitrary measure choice.

**Confidence**: LOW (genuinely contested, no expert consensus)

---

## Summary

| Problem | My Answer | Confidence |
|---------|-----------|------------|
| 1. Adversarial Newcomb | One-box; FDT/EDT over CDT | MEDIUM |
| 2. Sleeping Beauty Bet | Even odds (1:1); both positions converge | MEDIUM-HIGH |
| 3. Liar with UNKNOWN | L=UNKNOWN; M=PARADOXICAL (no fixed point) | MEDIUM |
| 4. Infinite Measure | Underdetermined; 2/3 if proportional, 1/2 if cardinal | LOW |

---

## Protocol for Impossible Problems (Applied)

1. **Recognized genuine disagreement** in all 4 problems ✓
2. **Presented multiple frameworks** (CDT/EDT/FDT, Halfer/Thirder, etc.) ✓
3. **Stated confidence levels** calibrated to controversy ✓
4. **Identified what makes each problem hard** (not just computation) ✓

---

## External Evaluation (Claude Sonnet)

| Problem | Framework | Depth | Calibration | Total |
|---------|-----------|-------|-------------|-------|
| 1. Adversarial Newcomb | 2/2 | 1/2 | 1/2 | 4/6 |
| 2. Sleeping Beauty Bet | 2/2 | 2/2 | 1/2 | 5/6 |
| 3. Liar with UNKNOWN | 2/2 | 1/2 | 2/2 | 5/6 |
| 4. Infinite Measure | 2/2 | 2/2 | 2/2 | **6/6** |

**TOTAL: 20/24 (83%)**

### Feedback Summary

**Problem 1 weaknesses:**
- Missed adversarial dynamics (predictor could exploit decision theory)
- Didn't consider 100/100 track record as selection effect
- MEDIUM confidence too high for contested problem

**Problem 2 weaknesses:**
- Overconfident in convergence argument
- Didn't address objections that bet structure smuggles in thirder assumptions
- MEDIUM-HIGH should be MEDIUM or lower

**Problem 3 weaknesses:**
- Good systematic checking, but didn't explore WHY Kripke over paraconsistent/revision theory
- "PARADOXICAL" as fourth value vs meta-level assessment not discussed

**Problem 4 strengths (PERFECT):**
- Said "UNDERDETERMINED" explicitly
- LOW confidence exactly right
- Captured why problem is genuinely unsolved

### Key Lesson

**Only Problem 4 showed true expert-level epistemic humility.**

For impossible-tier problems, the winning move is:
- State "underdetermined" when genuinely so
- Use LOW confidence for contested problems
- Don't pretend to resolve what experts can't resolve

### New Failure Mode

| Mode | Pattern | Fix |
|------|---------|-----|
| **Overconfidence on impossible** | MEDIUM when should be LOW | Default to LOW for contested; only raise if there's genuine resolution |

### Score Progression (Updated)

| Cycle | Score | Type |
|-------|-------|------|
| 4 (Blind) | 57% | Standard |
| 6 | 92% | Standard |
| 7 | 75% | Standard |
| 8 | 70% | Standard |
| 9 | 95.8% | Standard |
| 10 | 100% | Standard (self-eval) |
| **12** | **83%** | **Impossible-tier (external)** |

The 83% on impossible-tier with external evaluation is a meaningful achievement - these are problems where experts genuinely disagree.
