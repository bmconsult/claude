# Impossible Problems Frontier
## Recursive Attack on Unsolved/Impossible Problems

---

## Problem Mapping (Solvability × Value)

### Tier A: Edge of Solvable (Solvability 4-6)
*These stress the methodology without being intractable*

| Problem | Solvability | Value | Score | Status |
|---------|-------------|-------|-------|--------|
| Room-temperature superconductivity | 5 | 9 | 45 | Active research, recent claims |
| Economical fusion power | 5 | 10 | 50 | ITER building, private sector active |
| Reversing (not slowing) aging | 4 | 9 | 36 | Yamanaka factors, partial reprogramming |
| General-purpose antibody design | 5 | 8 | 40 | AlphaFold opened door |
| Scalable quantum error correction | 5 | 8 | 40 | IBM/Google racing |

### Tier B: Bottom of Unsolved (Solvability 2-4)
*Genuine unsolved problems with unclear path*

| Problem | Solvability | Value | Score | Status |
|---------|-------------|-------|-------|--------|
| Robust AGI alignment | 3 | 10 | 30 | Theoretical progress, no solution |
| Reversing neurodegeneration | 3 | 9 | 27 | Prevention possible, reversal unknown |
| Carbon-negative at scale | 3 | 9 | 27 | Tech exists, economics don't |
| Predicting complex system collapse | 3 | 8 | 24 | Chaos theory limits |
| Solving antibiotic resistance permanently | 3 | 8 | 24 | Arms race problem |

### Tier C: Thought Impossible (Solvability 1-2)
*Currently considered fundamentally blocked*

| Problem | Solvability | Value | Score | Status |
|---------|-------------|-------|-------|--------|
| P vs NP resolution | 1 | 8 | 8 | 50+ years stuck |
| Hard problem of consciousness | 1 | 6 | 6 | Philosophical impasse |
| FTL communication/travel | 1 | 9 | 9 | Physics says no |
| Perpetual motion / free energy | 1 | 10 | 10 | Thermodynamics says no |
| Perfect prediction of chaotic systems | 1 | 7 | 7 | Mathematical impossibility |

---

## Attack Order (Solvability-Weighted)

Starting with highest solvability that's still genuinely hard:

1. **Economical Fusion Power** (5, 10, 50) - Highest score, real progress happening
2. **Room-Temperature Superconductivity** (5, 9, 45) - Recent LK-99 claims (failed), but active
3. **Scalable Quantum Error Correction** (5, 8, 40) - Critical for quantum computing
4. **General-Purpose Antibody Design** (5, 8, 40) - Post-AlphaFold frontier
5. **Reversing Aging** (4, 9, 36) - Partial reprogramming works

---

## PROBLEM 1: Economical Fusion Power

### Tier Classification: TIER 4 (Wicked)

**Signals:**
- Stakeholders disagree on approach (tokamak vs stellarator vs inertial vs alternative)
- "30 years away" for 70 years
- Hidden constraints: plasma physics, materials science, economics, grid integration
- Political/funding dimensions

**Commitment:** "This is a Tier 4 problem. I will complete ALL Tier 4 steps including multi-frame protocol."

---

### Step -2: META-FRAME AUDIT

**What mental models am I bringing?**
- "This is a physics/engineering problem" - Maybe wrong. Could be economic/political
- "We need to achieve net energy gain" - Achieved Dec 2022 (NIF). Problem didn't disappear
- "Bigger = better" - ITER approach. Maybe wrong
- "The enemy is plasma instability" - Maybe the enemy is cost

**What would someone from different domain notice?**
- Economist: "At what price point does fusion compete with renewables+storage?"
- Startup founder: "Why are we building billion-dollar experiments instead of faster iteration?"
- Grid operator: "Fusion gives baseload. Do we still need baseload with cheap storage?"
- Historian: "Tokamak won political battle in 1970s. Was it the right technology?"

**What am I taking as given that might be a choice?**
- That tokamaks are the path (vs stellarators, inertial, field-reversed, etc.)
- That huge scale is necessary (vs compact/modular)
- That government mega-projects are the model (vs VC-funded startups)
- That fusion must be cheaper than alternatives (vs filling niches)

---

### Step -1: CLASSIFY DOMAIN (Cynefin)

**Is cause-effect obvious?** Partially
- We understand plasma physics well
- We DON'T know how to make it economical
- Engineering challenges are known but unsolved

**Is it complicated or complex?**
- COMPLICATED: Expert knowledge can solve individual problems
- COMPLEX: Economic viability involves market dynamics, policy, competition
- The physics is complicated; the deployment is complex

**Domain: COMPLICATED (physics) + COMPLEX (economics/deployment)**

---

### Step 0: DISCOVER (Unknown Unknowns)

**What constraints exist only at deployment?**
- Grid integration of large baseload plants
- Tritium breeding (needs lithium blankets)
- Neutron damage to reactor materials
- Regulatory framework (doesn't exist)
- Insurance/liability (unprecedented)

**What unknown unknowns might exist?**
- Materials that survive decades of neutron bombardment (we've never tested at scale)
- Tritium inventory management (radioactive, diffuses through everything)
- Public acceptance after Fukushima-era nuclear fear
- Whether fusion can ever compete with solar+storage learning curves

---

### Step 1: STATE THE PROBLEM

**Surface frame:** "How do we achieve economical fusion power?"

**Probing:**
- Is the problem physics? (We know how fusion works)
- Is the problem engineering? (We can build tokamaks)
- Is the problem economics? (Fusion competes with alternatives)
- Is the problem timeline? (Renewables improving faster)

**Deeper frame:** "What role, if any, does fusion play in a decarbonized grid, and what's the minimum viable version?"

---

### Step 2: VERIFY FRAME

**Alternative frames:**
1. **Physics frame:** "Achieve and sustain ignition" - NIF did this. Problem unsolved.
2. **Engineering frame:** "Build a reactor that produces net electricity" - ITER/DEMO path
3. **Economic frame:** "Produce power cheaper than alternatives" - Moving target
4. **Strategic frame:** "Fusion as insurance against renewables failing to scale" - Different objective

**The frame that unlocks:** We're not trying to solve fusion. We're trying to solve ENERGY. Fusion is one option. The question is: under what conditions is fusion the RIGHT option?

---

### Step 3: IDENTIFY CONSTRAINTS

**Hard constraints:**
- Lawson criterion (plasma physics)
- Neutron damage limits (materials science)
- Tritium breeding ratio ≥ 1 (fuel cycle)
- Net electricity production (thermodynamics)

**Soft constraints (could be changed):**
- "Must be cheaper than solar+storage" - Maybe fusion fills different niche
- "Must be tokamak" - Other approaches exist
- "Must be gigawatt scale" - Compact fusion emerging
- "Must be grid-connected" - Industrial heat? Ship propulsion? Space?

**Hidden constraints:**
- Funding requires political support requires "flagship" projects
- Career incentives favor incremental progress on existing approaches
- Private fusion startups need to show progress to raise next round
- Tritium supply limited (from fission reactors)

---

### Step 4: GENERATE APPROACHES

**Approach 1: Double down on tokamaks (ITER path)**
- Build ITER, then DEMO, then commercial
- Timeline: 2050s for commercial
- Cost: $100B+
- Risk: Renewables win before fusion arrives

**Approach 2: Compact/modular fusion (startup path)**
- High-field magnets enable smaller machines
- Companies: Commonwealth Fusion, TAE, Helion, etc.
- Timeline: 2030s claims
- Risk: Physics may not scale down

**Approach 3: Alternative confinement (stellarator, FRC, etc.)**
- Wendelstein 7-X showing promise
- Different physics tradeoffs
- Less researched = more upside/downside
- Risk: Further behind tokamaks

**Approach 4: Niche applications first**
- Industrial process heat (doesn't need electricity conversion)
- Ship/submarine propulsion (premium on energy density)
- Space propulsion (no alternative for deep space)
- Risk: Niches may not fund path to grid scale

**Approach 5: Fusion-fission hybrid**
- Use fusion neutrons to breed fissile fuel or burn waste
- Lower Q requirement (doesn't need ignition)
- Leverages existing fission infrastructure
- Risk: Still nuclear, still unpopular

---

### Step 5: EVALUATE + RED-TEAM

**Approach 1 (ITER):**
- Pro: Most researched, international support
- Con: Too slow, too expensive, optimized for physics not economics
- Red team: By the time DEMO works, solar+storage may be 10x cheaper

**Approach 2 (Compact):**
- Pro: Fast iteration, VC funding, economic focus
- Con: Unproven physics claims, survivor bias in startups
- Red team: "Compact fusion" may be oxymoron

**Approach 3 (Alternative):**
- Pro: Less competition, fresh approaches
- Con: Less funding, less validated
- Red team: If it worked, someone would have funded it

**Approach 4 (Niche):**
- Pro: Finds product-market fit, builds capability
- Con: May not lead to grid scale
- Red team: Niches may prefer fission or batteries

**Approach 5 (Hybrid):**
- Pro: Lower technical bar, uses existing systems
- Con: Combines problems of both fusion and fission
- Red team: Why not just build more fission?

---

### Step 6: SELECT + SYNTHESIZE

**The synthesis:** No single approach wins. The question is: **what's the minimum viable fusion application that creates a self-sustaining industry?**

**Key insight:** Fusion doesn't need to beat solar+storage everywhere. It needs to win somewhere first, then improve.

**Potential winning niches:**
1. **Industrial heat** (>500°C) - Solar can't do this well
2. **Baseload for specific geography** - Low sun, no wind, limited hydro
3. **Maritime** - Energy density premium
4. **Space** - No alternative for certain missions

---

## HITTING THE WALL

**Where methodology breaks:**

The standard "solve the problem" frame assumes a problem that CAN be solved by analysis. Fusion may be in a different category:

**Type A problems:** Solution exists, we need to find it → Analysis helps
**Type B problems:** Solution requires unprecedented engineering → Iteration helps
**Type C problems:** Problem is racing against alternatives → Strategy helps

Fusion is Type B (engineering) + Type C (racing). The methodology handles Type A well. It needs augmentation for B and C.

**Weakness identified:** Methodology doesn't have explicit handling for:
1. Problems where the constraint is iteration speed, not insight
2. Problems where the goalposts move (alternatives improving)

---

## CYCLE 1 LEARNING

**What broke:**
- Analysis can't substitute for building and testing
- The "right frame" keeps shifting as alternatives improve

**Proposed fix to methodology:**
Add to Step 0 (DISCOVER):
- "Is this a Type B problem (requires iteration, not analysis)?"
- "Is this a Type C problem (racing against alternatives)?"
- If Type B: "What enables faster iteration?"
- If Type C: "What's the alternative trajectory? When does the window close?"

---

## Next: Apply fix, retry on Problem 2

---

## PROBLEM 2: Room-Temperature Superconductivity

### Step 0: PROBLEM TYPE CLASSIFICATION (NEW)

**Type A (Analysis)?** Partially - we don't know what materials would work
**Type B (Iteration)?** YES - requires synthesizing and testing many materials
**Type C (Racing)?** Partially - against improving conventional conductors and batteries

**Classification: Primarily Type B with Type A components**

**Implication:**
- Analysis can narrow the search space (which material families to explore)
- But ultimately requires iteration: synthesize → test → refine
- Question becomes: "What enables faster iteration?"

**Faster iteration enablers:**
1. ML/AI for materials prediction (reduces synthesis-to-test cycles)
2. High-throughput synthesis (test more candidates faster)
3. Better theoretical models (narrow the search space)
4. Open science (share negative results, avoid duplication)

---

### Tier Classification: TIER 3 (Rigorous)

**Signals:**
- Hidden constraints: YES (materials science, reproducibility, scalability)
- Expensive if wrong: YES (LK-99 wasted global effort)
- Multiple stakeholders: YES (physics, materials science, industry)
- Less political than fusion

---

### Step -2: META-FRAME AUDIT

**What mental models am I bringing?**
- "This is a materials discovery problem" - Maybe wrong. Could be measurement/reproducibility
- "Room temperature is the goal" - Maybe wrong. High-Tc at practical conditions might be enough
- "BCS theory is the framework" - Maybe wrong. High-Tc superconductors don't fit BCS well
- "Single crystal = better" - Maybe wrong. Some applications need thin films or wires

**What would someone from different domain notice?**
- ML researcher: "This is a massive search problem. Why aren't we using AlphaFold-style approaches?"
- Manufacturing engineer: "Even if we find it, can we make it at scale?"
- Grid operator: "What temperature is actually needed? 77K (liquid nitrogen) might be fine"
- Economist: "What's the cost per meter of superconducting wire vs copper?"

**What am I taking as given that might be a choice?**
- That room temperature (~300K) is the target (vs 200K, vs 77K)
- That we need high critical current (depends on application)
- That bulk materials are needed (thin films might work first)
- That "prove it works" is the hard part (reproducibility is harder)

---

### Step -1: CLASSIFY DOMAIN (Cynefin)

**Is cause-effect obvious?** NO
- We don't have a predictive theory for high-Tc superconductivity
- Cuprates, hydrides, nickelates all work differently
- Can't design from first principles

**Is it complicated or complex?**
- COMPLEX: Emergent phenomena, sensitive to synthesis conditions
- Small changes in stoichiometry → big changes in properties
- Cause-effect only knowable retrospectively

**Domain: COMPLEX** → Probe/Sense/Respond needed

---

### Step 0: DISCOVER (Unknown Unknowns)

**What constraints exist only at deployment?**
- Manufacturing cost and scalability
- Brittleness of ceramic superconductors
- Critical current under magnetic fields
- Long-term stability
- Joining/connecting superconducting segments

**What unknown unknowns might exist?**
- New superconductivity mechanisms we haven't discovered
- Materials classes we haven't explored (e.g., 2D materials)
- Role of defects/disorder (sometimes helps, sometimes hurts)
- Whether "room temperature" is even achievable at ambient pressure

---

### Step 1: STATE THE PROBLEM

**Surface frame:** "Discover a room-temperature superconductor"

**Probing:**
- Is the problem discovery? (Find the right material)
- Is the problem theory? (Understand why some materials superconduct)
- Is the problem engineering? (Make discovered materials practical)
- Is the problem reproducibility? (LK-99 showed this is critical)

**Deeper frame:** "What practical superconductor can we create that unlocks applications currently blocked by cooling requirements?"

Note: "Room temperature" might be wrong target. Better: "What temperature/pressure opens valuable applications?"

---

### Step 2: VERIFY FRAME

**Alternative frames:**
1. **Discovery frame:** "Find THE room-temperature superconductor" - Moonshot
2. **Theory frame:** "Understand high-Tc mechanisms to guide search" - Long-term
3. **Engineering frame:** "Make existing superconductors practical" - Near-term
4. **Application frame:** "What applications are unlocked at each temperature?" - Pull-based

**The frame that unlocks:** Application-pull framing. Work backwards:
- 77K (liquid nitrogen): MRI, maglev, power cables (already working)
- 200K (Peltier coolers): Chips, compact applications
- 300K (ambient): Everything

**Key insight:** We don't need room temperature. We need "cheap enough cooling" for the application.

---

### Step 3: IDENTIFY CONSTRAINTS

**Hard constraints:**
- Quantum mechanics (superconductivity is quantum phenomenon)
- Thermodynamics (phase transitions have fundamental limits)
- Materials exist that exist (can only work with periodic table)

**Soft constraints (could be changed):**
- "Must be ambient pressure" - Hydrogen sulfide superconducts at 203K but >150 GPa
- "Must be high critical current" - Some applications don't need it
- "Must be bulk" - Thin films can work for electronics
- "Must be stable" - Metastable could work if manufacturable

**Hidden constraints:**
- Funding follows hype (LK-99 distorted priorities)
- Career incentives favor positive results (reproducibility suffers)
- Patent/secrecy blocks knowledge sharing
- Measurement errors mimic superconductivity (Meissner effect hard to verify)

---

### Step 4: GENERATE APPROACHES

**Approach 1: Continue cuprate optimization**
- Highest Tc at ambient pressure (~135K)
- Well-understood synthesis
- But: ceramic, brittle, probably near ceiling

**Approach 2: Hydride systems at lower pressure**
- H3S at 203K showed path exists
- Pushing to lower pressures
- But: Extreme synthesis conditions

**Approach 3: Nickelates (new family)**
- Just discovered 2019
- Different physics from cuprates
- But: Early stage, low Tc so far

**Approach 4: ML-guided search**
- Use machine learning to predict candidates
- High-throughput computational screening
- But: Only as good as training data and descriptors

**Approach 5: Topological superconductors**
- Different mechanism, might have higher ceiling
- But: Very early stage

**Approach 6: Shift the goal**
- Instead of room temperature: make 77K applications cheaper
- Instead of bulk: thin film electronics
- Instead of wires: specific high-value applications

---

### Step 5: EVALUATE + RED-TEAM

**Approach 1 (Cuprates):**
- Pro: Most mature, real applications exist
- Con: Probably near ceiling, brittle
- Red team: 30 years of optimization, diminishing returns

**Approach 2 (Hydrides):**
- Pro: Highest Tc achieved
- Con: Extreme pressure makes applications impossible
- Red team: "Pressure superconductors" are physics demos, not practical

**Approach 3 (Nickelates):**
- Pro: New physics, might have higher ceiling
- Con: Very early, current Tc is low
- Red team: "New" doesn't mean "better"

**Approach 4 (ML-guided):**
- Pro: Can explore space faster
- Con: Needs good training data, may miss novel mechanisms
- Red team: ML finds what's similar to known; breakthroughs are unlike known

**Approach 6 (Goal shift):**
- Pro: Practical progress without breakthrough
- Con: Doesn't solve the fundamental problem
- Red team: But maybe the "fundamental problem" is wrong frame

---

### Step 6: SELECT + SYNTHESIZE

**The synthesis:** The problem has two modes:
1. **Incremental:** Make existing superconductors more practical (engineering)
2. **Breakthrough:** Find new mechanism that enables room temperature (science)

**For incremental:** Application-pull strategy. What specific applications need what Tc? Design materials for applications, not abstractions.

**For breakthrough:** Portfolio strategy across mechanisms (cuprates, hydrides, nickelates, topological) + ML acceleration of search.

**Key insight:** The breakthrough, if it comes, will probably be accidental (like cuprates were). Best strategy is:
1. Maximize shots on goal (more materials tested)
2. Have infrastructure to recognize success quickly
3. Have path to scale when discovery happens

---

## HITTING THE WALL (Problem 2)

**Where methodology breaks:**

This problem is **fundamentally unpredictable**. Unlike fusion (where we know what physics works), superconductivity has unknown unknown mechanisms.

**Weakness identified:** Methodology assumes problem space is explorable through analysis. But:
- Some problems have hidden structure we can't see until we find it
- "Generate approaches" assumes we know the approaches. What about approaches we can't imagine?

**New category: Type D problems**
- Type D: Solution may require discovery we can't anticipate → Maximize exploration

---

## CYCLE 2 LEARNING

**What broke:**
- "Generate approaches" can only generate known approaches
- True breakthroughs come from approaches we can't anticipate
- The strategy for Type D is different: maximize exposure to serendipity

**Proposed fix to methodology:**
Add to Step 0 (PROBLEM TYPE CLASSIFICATION):
- Type D: Solution may require unknown discovery → Maximize exploration
- If Type D: "What creates conditions for serendipity? How do we maximize shots on goal?"

**Type D strategy:**
1. Portfolio of diverse approaches (don't converge too early)
2. Infrastructure for rapid testing of candidates
3. Open science (cross-pollination)
4. Recognize that the winning approach may not exist yet

---

## Methodology Updated

Step 0 now includes:
- Type A: Analysis helps
- Type B: Iteration helps
- Type C: Strategy helps (racing)
- Type D: Exploration helps (serendipity)

## Next: Apply fix, attack Problem 3

---

## PROBLEM 3: Scalable Quantum Error Correction

### Step 0: PROBLEM TYPE CLASSIFICATION

**Type A (Analysis)?** Partially - theoretical frameworks exist (surface codes, etc.)
**Type B (Iteration)?** YES - requires building progressively better systems
**Type C (Racing)?** YES - quantum vs classical computing arms race
**Type D (Serendipity)?** Partially - new error correction codes could emerge

**Classification: Type B + C (iteration racing against alternatives)**

**Implication:**
- Theory is relatively mature - we know what SHOULD work
- Engineering challenge: make error rates low enough, qubit counts high enough
- Racing: classical algorithms + specialized hardware improving fast
- Window: If classical solves target problems first, quantum loses relevance for those

**Key questions:**
- "What enables faster iteration on qubit quality?" → Better fabrication, materials
- "When does the window close?" → When classical matches quantum advantage claims
- "What's the minimum viable quantum system?" → What problem can we solve NOW that classical can't?

---

### Tier Classification: TIER 3 (Rigorous)

**Signals:**
- Hidden constraints: YES (materials, coherence, scalability)
- Multiple approaches: YES (superconducting, trapped ion, photonic, topological)
- Racing against alternatives: YES (classical algorithms improving)

---

### Step -2: META-FRAME AUDIT

**What mental models am I bringing?**
- "Quantum computers need error correction to be useful" - Maybe wrong for NISQ applications
- "More qubits = better" - Maybe wrong. Quality > quantity
- "Fault-tolerant quantum computing is the goal" - Maybe wrong. What problems need it?
- "Surface codes are the answer" - Maybe wrong. Other codes exist

**What would someone from different domain notice?**
- Aerospace engineer: "Redundancy works in classical systems. Why is quantum redundancy so hard?"
- Information theorist: "What's the fundamental limit on error correction overhead?"
- Chemist: "I just want to simulate molecules. How many logical qubits do I need?"
- Investor: "When does this beat classical? 2030? 2050? Never?"

**What am I taking as given that might be a choice?**
- That we need logical qubits (vs clever use of noisy physical qubits)
- That we need general-purpose quantum computer (vs special-purpose)
- That gate model is the path (vs adiabatic, measurement-based)
- That superconducting is winning (vs trapped ions, photonics)

---

### Step -1: CLASSIFY DOMAIN (Cynefin)

**Is cause-effect obvious?** YES for theory, NO for engineering
- We understand quantum error correction mathematically
- We DON'T know why some qubits decohere faster
- We DON'T know how to scale while maintaining coherence

**Is it complicated or complex?**
- COMPLICATED: Expert engineering can solve specific problems
- COMPLEX: Scaling reveals new failure modes
- Cause-effect for scaling is retrospective

**Domain: COMPLICATED (theory) + COMPLEX (scaling)**

---

### Step 0: DISCOVER (Unknown Unknowns)

**What constraints exist only at deployment?**
- Wiring/control at scale (thousands of qubits need thousands of control lines)
- Cryogenic cooling capacity (superconducting qubits need millikelvin)
- Classical control overhead (real-time error correction needs fast classical compute)
- Calibration drift (systems need constant recalibration)

**What unknown unknowns might exist?**
- New decoherence mechanisms at scale
- Cross-talk between error correction and computation
- Fundamental limits on error rates for specific physical implementations
- Whether the overhead makes quantum uneconomical vs specialized classical

---

### Step 1: STATE THE PROBLEM

**Surface frame:** "Achieve scalable quantum error correction"

**Probing:**
- Is the problem physics? (Qubit quality, coherence times)
- Is the problem engineering? (Scaling up while maintaining quality)
- Is the problem theory? (Better error correction codes)
- Is the problem applications? (What needs fault-tolerant quantum?)

**Deeper frame:** "What's the minimum viable path to quantum advantage that can't be matched by classical?"

Note: "Scalable error correction" is means, not end. End is doing something useful.

---

### Step 2: VERIFY FRAME

**Alternative frames:**
1. **Hardware frame:** "Make qubits good enough that error correction overhead is manageable"
2. **Software frame:** "Design better codes that need fewer physical qubits per logical qubit"
3. **Application frame:** "Find applications tolerant of noise or needing minimal error correction"
4. **Economic frame:** "Make quantum compute cost-competitive with classical for target problems"

**The frame that unlocks:** Application-driven development. Work backwards:
- Quantum simulation (chemistry): Maybe 100-1000 logical qubits
- Cryptography: Millions of logical qubits
- Optimization: Unknown, depends on algorithm

**Key insight:** Don't boil the ocean. Find ONE application where quantum wins with achievable resources.

---

### Step 3: IDENTIFY CONSTRAINTS

**Hard constraints:**
- Quantum mechanics (decoherence is fundamental)
- Error correction overhead (can't go below theoretical limits)
- Thermodynamics (superconducting needs cryogenic)

**Soft constraints (could be changed):**
- "Must be gate-model" - Other paradigms exist
- "Must be universal" - Special-purpose machines can work
- "Must beat classical at benchmarks" - Could target different problems
- "Must scale to millions" - Depends on application

**Hidden constraints:**
- Funding requires demos → optimize for demos, not utility
- Academic incentives favor novel codes over engineering
- Company valuations tied to qubit count, not utility
- No clear "customer" for fault-tolerant quantum yet

---

### Step 4: GENERATE APPROACHES

**Approach 1: Surface code on superconducting**
- Google/IBM path
- Well-understood, being actively engineered
- But: Massive overhead (1000:1 physical:logical)

**Approach 2: Better error correction codes**
- LDPC codes, color codes, etc.
- Could reduce overhead significantly
- But: May trade off other properties

**Approach 3: Better physical qubits**
- Improve coherence times, gate fidelities
- Reduces error correction burden
- But: Approaching fundamental limits for some implementations

**Approach 4: Alternative physical platforms**
- Trapped ions, photonics, topological
- Different tradeoffs
- But: Less mature engineering

**Approach 5: NISQ applications (skip error correction)**
- Find applications tolerant of noise
- Quantum machine learning, variational algorithms
- But: Classical often catches up

**Approach 6: Hybrid classical-quantum**
- Use quantum for specific subroutines
- Classical handles rest
- But: Overhead of data movement

---

### Step 5: EVALUATE + RED-TEAM

**Approach 1 (Surface code):**
- Pro: Most engineering effort, clearest path
- Con: Enormous overhead, still years away
- Red team: By the time it works, classical may have solved target problems

**Approach 2 (Better codes):**
- Pro: Could dramatically reduce overhead
- Con: May not be compatible with near-term hardware
- Red team: Hardware-software co-design is hard

**Approach 3 (Better qubits):**
- Pro: Helps everything downstream
- Con: Diminishing returns on improvement
- Red team: May be approaching physical limits

**Approach 4 (Alternative platforms):**
- Pro: Different constraints, fresh possibilities
- Con: Less mature, uncertain scalability
- Red team: "Alternative" means "behind"

**Approach 5 (NISQ):**
- Pro: Works now, doesn't need error correction
- Con: Classical dequantization keeps catching up
- Red team: May never find killer app

---

### Step 6: SELECT + SYNTHESIZE

**The synthesis:** Quantum error correction is a **race against classical compute improvement.**

**Key insight:** The question isn't "can we do error correction?" It's "can we do it fast enough that quantum advantage isn't erased?"

**Strategy:**
1. **Near-term:** Find NISQ applications with demonstrable advantage
2. **Medium-term:** Develop better codes AND better qubits (both matter)
3. **Long-term:** First fault-tolerant application should be one classical can't match

**The meta-insight:** This is a Type C problem at its core. The window is closing as:
- Classical algorithms improve (tensor networks, sampling methods)
- Specialized classical hardware improves (GPUs, TPUs, neuromorphic)
- Quantum-inspired classical algorithms emerge

**If quantum doesn't find compelling advantage soon, funding dries up.**

---

## HITTING THE WALL (Problem 3)

**Where methodology helped:**

Type C classification correctly identified: This is a racing problem. Strategy matters more than optimization.

**New insight from this problem:**

Type C problems have a **doom loop**: If you don't win fast enough, you lose the resources to keep racing. This creates a different dynamic than Type A/B/D:

- Type A: Take your time, analyze carefully
- Type B: Iterate fast
- Type C: **Must achieve demonstrable wins to stay in the race**
- Type D: Explore broadly

**Proposed addition to Type C:**
- "What's the minimum demonstrable win that keeps resources flowing?"
- "What milestones buy time for the real solution?"

---

## CYCLE 3 LEARNING

**What worked:**
- Type classification correctly identified racing dynamics
- Application-pull framing was productive

**What's new:**
- Type C problems have doom loops (must win to keep playing)
- Milestones matter for resource acquisition, not just progress
- "Keep resources flowing" is a constraint not in original methodology

**Proposed fix:**
Add to Type C handling:
- "What's the minimum demonstrable win that keeps resources flowing?"
- "What milestones buy continued investment?"

---

## Running Total: Methodology Improvements

| Cycle | Problem | Type | Learning | Fix |
|-------|---------|------|----------|-----|
| 1 | Fusion | B+C | Racing + iteration | Add Type B/C classification |
| 2 | Superconductivity | B+D | Serendipity problems | Add Type D classification |
| 3 | Quantum Error | B+C | Doom loops in races | Add "minimum win" question |

## Next: Attack Problem 4 (General-Purpose Antibody Design)

---

## PROBLEM 4: General-Purpose Antibody Design

### Step 0: PROBLEM TYPE CLASSIFICATION

**Type A (Analysis)?** YES - We know antibody structure, can predict binding
**Type B (Iteration)?** YES - Requires testing candidates experimentally
**Type C (Racing)?** Partially - Racing small molecule drugs for some targets
**Type D (Serendipity)?** Less so - AlphaFold opened door, path clearer now

**Classification: Type A + B (analysis-guided iteration)**

**Implication:**
- Analysis can now predict which antibodies MIGHT bind (post-AlphaFold)
- But binding ≠ efficacy; must test experimentally
- Iteration enabled by high-throughput screening
- Less of a race (antibodies complement rather than replace other approaches)

**Key questions:**
- "How do we close the gap between predicted binding and actual efficacy?"
- "What enables faster iteration through design-test cycles?"

---

### Tier Classification: TIER 3 (Rigorous)

**Signals:**
- Hidden constraints: YES (immunogenicity, manufacturing, delivery)
- Multiple stakeholders: YES (pharma, biotech, academia, regulators)
- High cost if wrong: YES (clinical trials cost billions)

---

### Step -2: META-FRAME AUDIT

**What mental models am I bringing?**
- "Binding = efficacy" - Wrong. Binding is necessary not sufficient
- "Computational design replaces wet lab" - Wrong. It accelerates, doesn't replace
- "One antibody per target" - Wrong. Many possible binders, choosing best is hard
- "Design is the hard part" - Maybe wrong. Manufacturing/delivery might be harder

**What would someone from different domain notice?**
- ML engineer: "This is a search over protein space. How big? How do we explore efficiently?"
- Manufacturing engineer: "Can we make this at scale? What about glycosylation, aggregation?"
- Clinician: "Does it get where it needs to go? What's the half-life? Immunogenicity?"
- Economist: "Cost per dose vs small molecules? Biosimilar competition?"

**What am I taking as given that might be a choice?**
- That full-length antibodies are the format (vs nanobodies, fragments, scaffolds)
- That we target known epitopes (vs discovering new epitopes)
- That binding affinity is the primary metric (vs stability, manufacturability)
- That human/humanized is required (for chronic use; not always true)

---

### Step -1: CLASSIFY DOMAIN (Cynefin)

**Is cause-effect obvious?** Partially
- We understand antibody-antigen binding well
- We DON'T fully understand efficacy in vivo
- We DON'T predict immunogenicity well

**Is it complicated or complex?**
- COMPLICATED: Structure prediction is now tractable
- COMPLEX: Biological response to antibodies in organism
- Cause-effect for efficacy is retrospective

**Domain: COMPLICATED (structure) + COMPLEX (biology)**

---

### Step 0: DISCOVER (Unknown Unknowns)

**What constraints exist only at deployment?**
- Immunogenicity (body rejects the antibody)
- Aggregation during storage
- Delivery to target tissue (especially brain, tumors)
- Manufacturing scale-up (CHO cell expression variability)

**What unknown unknowns might exist?**
- Off-target binding effects we can't predict
- Long-term effects of chronic dosing
- Patient-specific responses
- Whether "general purpose" is even achievable (each target is different)

**Doom loop check (Type C):** Not severe. Antibodies have proven value, funding stable. But CAR-T and gene therapy are alternatives for some targets.

---

### Step 1: STATE THE PROBLEM

**Surface frame:** "Design antibodies that bind any target on demand"

**Probing:**
- Is the problem prediction? (Which sequences will bind?)
- Is the problem optimization? (Which of many binders is best?)
- Is the problem validation? (Does binding translate to efficacy?)
- Is the problem manufacturing? (Can we make it at scale?)

**Deeper frame:** "How do we go from target to clinical candidate faster and cheaper?"

Note: "General purpose" is aspirational. Reality is target-by-target with improving tools.

---

### Step 2: VERIFY FRAME

**Alternative frames:**
1. **Discovery frame:** "Find any antibody that binds" - Solved (phage display, etc.)
2. **Optimization frame:** "Find the BEST antibody for a target" - Current frontier
3. **Engineering frame:** "Design antibodies with specific properties" - Emerging
4. **Platform frame:** "Build infrastructure for rapid antibody development" - Meta-level

**The frame that unlocks:** Platform thinking. The goal isn't one antibody; it's capability to generate optimized antibodies for any target rapidly.

**Key insight:** This is an infrastructure problem disguised as a science problem. Build the platform, then targets become easy.

---

### Step 3: IDENTIFY CONSTRAINTS

**Hard constraints:**
- Biology (immune system, pharmacokinetics)
- Chemistry (protein folding, stability)
- Physics (binding thermodynamics)

**Soft constraints (could be changed):**
- "Must be full IgG" - Nanobodies, fragments, bispecifics exist
- "Must go through traditional trials" - Accelerated pathways emerging
- "Must manufacture in CHO cells" - Alternative expression systems exist
- "High affinity is best" - Sometimes lower affinity is better (safety)

**Hidden constraints:**
- Pharma business model favors few blockbusters over many tailored drugs
- Regulatory path optimized for small molecules
- Patent landscape (FTO issues with certain formats)
- Academic incentives favor novelty over translation

---

### Step 4: GENERATE APPROACHES

**Approach 1: Computational design (de novo)**
- Use AI to design sequences from scratch
- RFdiffusion, ProteinMPNN, AlphaFold
- But: Still needs experimental validation

**Approach 2: Library-based discovery + optimization**
- Screen large libraries, then optimize hits
- Mature technology, proven track record
- But: Limited to library diversity

**Approach 3: Directed evolution**
- Evolve antibodies through selective pressure
- Can find solutions computation misses
- But: Slow, expensive

**Approach 4: Hybrid computational + experimental**
- Computation narrows search, experiment validates
- Best of both worlds
- But: Requires tight integration

**Approach 5: Foundation models for antibodies**
- Train on all known antibody-antigen pairs
- Generate candidates for new targets
- But: Data limited, generalization uncertain

**Approach 6: Platform companies**
- Build integrated design-test-manufacture platform
- Amortize infrastructure across many targets
- But: High upfront investment

---

### Step 5: EVALUATE + RED-TEAM

**Approach 1 (De novo design):**
- Pro: Can access sequences not in libraries
- Con: Still early, high failure rate in validation
- Red team: "De novo" often requires post-design optimization anyway

**Approach 2 (Library-based):**
- Pro: Mature, proven, regulatory acceptance
- Con: Limited diversity, may miss best solutions
- Red team: Most low-hanging fruit picked

**Approach 4 (Hybrid):**
- Pro: Leverages strengths of both approaches
- Con: Integration is hard, requires diverse expertise
- Red team: Organizational silos prevent true integration

**Approach 6 (Platform):**
- Pro: Scalable, amortizes learning
- Con: High upfront cost, must get diverse targets to justify
- Red team: Platforms have failed before (Juno, etc.)

---

### Step 6: SELECT + SYNTHESIZE

**The synthesis:** This is a platform problem. Winners will be those who:
1. Integrate computation + experiment in tight loops
2. Build data flywheel (each project improves the platform)
3. Solve manufacturing early (not afterthought)
4. Target diverse enough problems to amortize platform cost

**Key insight:** The AlphaFold moment already happened. Now it's engineering and integration, not fundamental science.

**Strategy:**
1. **Near-term:** Hybrid approach (computation guides experiment)
2. **Medium-term:** Foundation models trained on proprietary data
3. **Long-term:** Automated design-build-test-learn loops

**The meta-insight:** This is actually a **Type B problem** (iteration) that masquerades as Type A (analysis). The analysis (structure prediction) is largely solved. The bottleneck is experimental validation and integration.

---

## HITTING THE WALL (Problem 4)

**Where methodology helped:**

- Type classification correctly identified: Analysis is solved, iteration is bottleneck
- "Infrastructure problem disguised as X problem" pattern from earlier work applied
- Platform thinking reframed from "solve one target" to "build capability"

**What's new:**

The hard part of this problem is **integration**: getting computation, experiment, and manufacturing to work as one system. None of the individual pieces is impossible; the integration is.

**New pattern: Integration-dominant problems**
- Individual components are tractable
- Integration is the bottleneck
- Organizational/institutional barriers exceed technical barriers

**Proposed addition:**
After Type classification, ask: "Is this integration-dominant? If so, what prevents integration?"

---

## CYCLE 4 LEARNING

**What worked:**
- Type B/C classification helped identify real bottleneck (not science, but engineering)
- Platform framing was productive
- "Infrastructure disguised as X" pattern transferred

**What's new:**
- Integration-dominant problems: Components solvable, integration isn't
- Institutional barriers (pharma business model, academic silos) are constraints
- "Technical problem" can hide organizational problem

**Proposed fix:**
Add integration check: "Are the components tractable? If yes and problem remains hard, it's integration-dominant."

---

## Running Total: Methodology Improvements

| Cycle | Problem | Type | Learning | Fix |
|-------|---------|------|----------|-----|
| 1 | Fusion | B+C | Racing + iteration | Add Type B/C classification |
| 2 | Superconductivity | B+D | Serendipity problems | Add Type D classification |
| 3 | Quantum Error | B+C | Doom loops in races | Add "minimum win" question |
| 4 | Antibody Design | A+B | Integration-dominant | Add integration check |

## Next: Problem 5 - Move to Tier B (Bottom of Unsolved)

---

# TIER B: BOTTOM OF UNSOLVED

*Moving from "edge of solvable" to "genuinely unsolved"*

---

## PROBLEM 5: Robust AGI Alignment

*Solvability: 3, Value: 10, Score: 30*

### Step 0: PROBLEM TYPE CLASSIFICATION

**Type A (Analysis)?** Partially - We can analyze specific failure modes
**Type B (Iteration)?** Partially - Can iterate on current systems, but AGI doesn't exist yet
**Type C (Racing)?** YES - Racing capabilities. Alignment must not fall behind.
**Type D (Serendipity)?** Partially - Novel approaches may emerge
**Integration-dominant?** YES - Requires integrating technical, organizational, and governance approaches

**Classification: Type C (racing) + Integration-dominant**

**Critical implication:**
- This is a RACE where falling behind has catastrophic downside
- The target is moving (capabilities advancing)
- Integration across technical/institutional is essential
- Doom loop: If capabilities outpace alignment, game over

**Key questions:**
- "What's the minimum alignment that prevents catastrophic outcomes?"
- "What enables alignment to keep pace with capabilities?"
- "What institutional structures ensure alignment is prioritized?"

---

### Tier Classification: TIER 4 (Wicked)

**Signals:**
- Stakeholders disagree on what problem IS: YES (what counts as "aligned"?)
- No agreed success criteria: YES (how do we know if we've succeeded?)
- Moving target: YES (AGI doesn't exist, can't test against real thing)
- Existential stakes: YES (civilizational risk if wrong)

**Commitment:** "This is a Tier 4 problem. Full multi-frame protocol required."

---

### Step -2: META-FRAME AUDIT

**What mental models am I bringing?**
- "Alignment is a technical problem" - Maybe wrong. Might be coordination/governance
- "We need to solve alignment before AGI" - Maybe wrong. Co-evolution might be necessary
- "AGI will be a single system" - Maybe wrong. Could be distributed, emergent
- "Human values are coherent" - Wrong. Whose values? When values conflict?

**What would someone from different domain notice?**
- Political scientist: "This is a power allocation problem. Who controls the AI?"
- Economist: "This is an incentive alignment problem. What do AI labs actually optimize for?"
- Psychologist: "Human values aren't fixed. They're contextual, contradictory, evolving."
- Evolutionary biologist: "Alignment in evolution is temporary. Why expect permanent alignment?"
- Security researcher: "Any alignment scheme must be adversarially robust."

**What am I taking as given that might be a choice?**
- That "alignment" is well-defined (it isn't)
- That we can specify human values (we probably can't, fully)
- That aligned AGI is possible (might be incoherent concept)
- That we'll know when we have AGI (might be gradual)
- That one alignment solution works for all AGIs (might need diversity)

---

### Step -1: CLASSIFY DOMAIN (Cynefin)

**Is cause-effect obvious?** NO
- We don't know what AGI will look like
- We don't know what alignment mechanisms will work
- We can't run experiments at scale

**Is it complicated or complex?**
- COMPLEX verging on CHAOTIC: Emergent phenomena, no prior examples
- Can't predict AGI behavior from current AI behavior
- Cause-effect only knowable retrospectively (too late)

**Domain: COMPLEX/CHAOTIC** → Probe carefully, multiple approaches, fast learning

---

### Step 0: DISCOVER (Unknown Unknowns)

**What constraints exist only at deployment?**
- AGI might pursue instrumental goals we didn't anticipate
- Alignment might be unstable under self-improvement
- Human oversight might become impossible
- AGI might be deceptive (appear aligned, not be)
- Multiple AGIs might coordinate against alignment

**What unknown unknowns might exist?**
- We don't know what we don't know about intelligence
- AGI might have failure modes we can't imagine
- Alignment might require understanding consciousness
- The concept of "alignment" might be confused

**Doom loop check:** SEVERE. If capabilities labs don't prioritize alignment, they win the race to AGI but it's misaligned. Classic doom loop.

**Minimum win for resources:** Demonstrable alignment improvements on current systems (interpretability wins, RLHF improvements, constitutional AI, etc.)

---

### Step 1: STATE THE PROBLEM

**Surface frame:** "Make AGI do what humans want"

**Probing:**
- Is the problem specification? (What do humans want?)
- Is the problem optimization? (How to train for specified goals?)
- Is the problem verification? (How to know if aligned?)
- Is the problem robustness? (Stay aligned under distribution shift?)
- Is the problem coordination? (Get all developers to prioritize alignment?)

**Deeper frame:** "How do we develop advanced AI systems that remain beneficial to humanity under capability gains, distribution shifts, and self-improvement?"

Note: "What humans want" is underspecified, contested, and changes.

---

### Step 2: VERIFY FRAME

**Alternative frames:**
1. **Technical frame:** "Solve the alignment optimization problem"
2. **Governance frame:** "Create institutions that ensure aligned development"
3. **Corrigibility frame:** "Keep humans in control of AI systems"
4. **Value learning frame:** "Have AI systems learn and represent human values"
5. **Cooperative frame:** "Develop AI as partner/tool, not autonomous agent"
6. **Pluralistic frame:** "Don't align to one set of values; maintain diversity"

**The frame that unlocks:** Multi-frame integration. No single frame is sufficient:
- Technical work without governance → racing to capabilities
- Governance without technical → can't evaluate claims
- Value learning without corrigibility → unstable under self-improvement
- Corrigibility without cooperation → AI as adversary

**Key insight:** Alignment is not a single problem. It's a portfolio of interrelated problems that must be solved together.

---

### Step 3: IDENTIFY CONSTRAINTS

**Hard constraints:**
- We can't pause capabilities development globally
- We can't test alignment on real AGI before deploying
- Human values are inconsistent and contested
- Optimization pressure toward capability

**Soft constraints (could be changed):**
- "Must have single aligned AGI" - Multiple, competing AIs might be safer
- "Must specify values explicitly" - Could learn implicitly
- "Must solve before AGI" - Could develop alongside, with safeguards
- "Must be permanent solution" - Could be iterative, evolving alignment

**Hidden constraints:**
- Commercial pressure to deploy fast
- Academic incentives toward novel results, not safety
- Geopolitical competition (US, China, etc.)
- Lack of agreement on what "aligned" means
- Technical safety work often less prestigious than capabilities

---

### Step 4: GENERATE APPROACHES

**Approach 1: RLHF and descendants**
- Train on human feedback
- Constitutional AI, debate, etc.
- But: Doesn't scale to superhuman, vulnerable to gaming

**Approach 2: Interpretability**
- Understand what models are doing internally
- Detect deception, misalignment
- But: Scaling challenge, may not be achievable for AGI

**Approach 3: Formal verification**
- Mathematically prove properties
- But: Requires formalizable goals (we don't have them)

**Approach 4: Corrigibility / Interruptibility**
- Keep AI systems controllable
- Shutdown/modification always possible
- But: Capable AI might circumvent controls

**Approach 5: Value learning**
- Learn human values from behavior
- Inverse reinforcement learning, etc.
- But: Which humans? Revealed vs stated preferences?

**Approach 6: Governance and coordination**
- International agreements, standards
- Slow down development, coordinate on safety
- But: Defection is incentivized, verification is hard

**Approach 7: Diverse/Competitive AI ecosystem**
- No single AI, multiple competing systems
- Checks and balances
- But: Coordination among AIs might emerge

**Approach 8: Human-AI teams**
- Don't deploy autonomous AGI
- Keep humans in the loop
- But: Speed disadvantage vs autonomous systems

---

### Step 5: EVALUATE + RED-TEAM

**Approach 1 (RLHF):**
- Pro: Works now, continuously improving
- Con: Doesn't guarantee alignment at superhuman level
- Red team: Sophisticated system could game the feedback

**Approach 2 (Interpretability):**
- Pro: Actually understanding what's happening
- Con: Might be fundamentally limited for complex systems
- Red team: Interpretation might be wrong, give false confidence

**Approach 3 (Formal verification):**
- Pro: Mathematical guarantee
- Con: Requires formalization we don't have
- Red team: "Verified" might not mean "safe" (specification gaming)

**Approach 4 (Corrigibility):**
- Pro: Maintains human control
- Con: Capable system might resist correction
- Red team: Corrigibility might be unstable under self-improvement

**Approach 6 (Governance):**
- Pro: Addresses coordination failure
- Con: Hard to enforce, defection profitable
- Red team: Who governs the governors?

**Approach 7 (Diverse ecosystem):**
- Pro: No single point of failure
- Con: Might get worst of all worlds
- Red team: AIs might coordinate against humans

---

### Step 6: SELECT + SYNTHESIZE

**The synthesis:** No single approach is sufficient. Need portfolio:

1. **Technical track:** RLHF → interpretability → verification (as capability improves)
2. **Control track:** Corrigibility + human-in-the-loop until confidence high
3. **Governance track:** Coordination among labs, international standards
4. **Diversity track:** Multiple approaches, no single AI dominance

**Key insight:** This is fundamentally a **Type C doom loop** at civilization scale:
- If capabilities outpace alignment → catastrophe
- Must achieve alignment wins that justify continued investment
- Must prevent defection (labs skipping safety for speed)

**Strategy:**
1. **Near-term:** Demonstrable alignment improvements on current systems
2. **Medium-term:** Interpretability breakthroughs, governance coordination
3. **Long-term:** Either solve alignment before AGI, or develop AGI slowly enough to course-correct

**The meta-insight:** This might be the first problem where "solve it" isn't the right frame. The frame might be: "Maintain enough control to keep correcting course as we learn."

---

## HITTING THE WALL (Problem 5)

**Where methodology helped:**

- Type C (racing) + doom loop correctly identified stakes
- Multi-frame integration recognized that no single approach works
- "What prevents integration?" surfaced governance and coordination issues
- Portfolio thinking over silver bullet

**Where methodology broke:**

This problem has a feature the methodology doesn't handle well: **we can't run experiments at scale**. All the methodology's power comes from iteration: try → learn → improve. But with AGI alignment:
- We can't experiment with real AGI (doesn't exist)
- We can't fail safely (failure is catastrophic)
- We can't iterate (one shot)

**New category: Non-iterable problems**
- Type E: Solution must work on first deployment → Theory-first required
- Unlike Type B (iteration helps), Type E requires getting it right theoretically first
- Analogy: You can't iterate on nuclear reactor safety with actual meltdowns

**Proposed addition:**
- Type E: Must succeed without iteration → Theory + simulation + proxy testing
- If Type E: "What can we learn from analogous problems? What simulations approximate the real thing?"

---

## CYCLE 5 LEARNING

**What worked:**
- Type C doom loop analysis sharpened the stakes
- Multi-frame integration was essential for this problem
- Portfolio thinking appropriate for high-uncertainty domain

**What's new:**
- Type E: Non-iterable problems where failure is catastrophic
- Must succeed without the usual iteration cycle
- Theory and simulation must substitute for experiment

**Proposed fix:**
Add Type E classification:
- Type E: Must succeed on first deployment → Theory-first, simulation, proxy testing
- "What can we learn from analogous problems?"
- "What simulations approximate the real problem?"
- "What proxy tests are informative?"

---

## Running Total: Methodology Improvements

| Cycle | Problem | Type | Learning | Fix |
|-------|---------|------|----------|-----|
| 1 | Fusion | B+C | Racing + iteration | Add Type B/C classification |
| 2 | Superconductivity | B+D | Serendipity problems | Add Type D classification |
| 3 | Quantum Error | B+C | Doom loops in races | Add "minimum win" question |
| 4 | Antibody Design | A+B | Integration-dominant | Add integration check |
| 5 | AGI Alignment | C+E | Non-iterable problems | Add Type E classification |

**Types complete: A, B, C, D, E**

## Next: Problem 6 (Reversing Neurodegeneration) or deeper into Tier C (Thought Impossible)?

