{
  "old_criteria": "\nCRITERIA (each 0-3, total 15):\n1. TENSION IDENTIFICATION (0-3)\n2. MECHANISM SPECIFICITY (0-3)\n3. FAILURE AWARENESS (0-3)\n4. CONSTRAINT SATISFACTION (0-3)\n5. ACTIONABILITY (0-3)\n",
  "new_criteria": "# APPLYING PROBLEM-SOLVING STRATEGY v3.0\n\n## 1. FRAME\n**Problem type:** Systems/analytical - redesigning measurement systems\n**Success looks like:** New rubric that differentiates quality levels and captures what makes solutions truly excellent vs merely competent\n**Hard constraints:** 5 criteria, 0-3 scale each, must be measurable and quote-based\n\n## 2. GENERATE\n\n**Approach A - Depth & Sophistication Focus**\n- Measure conceptual depth, systems thinking, evidence quality, scalability, transferability\n\n**Approach B - Implementation Reality Check** \n- Focus on practical execution barriers, resource requirements, stakeholder dynamics, timeline realism, measurement specificity\n\n**Approach C - Robustness & Adaptability**\n- Evaluate solution resilience, edge case handling, failure recovery, context sensitivity, future-proofing\n\n## 3. EVALUATE\n\n**Approach A:** Addresses core tension (\u2713), could fail by being too abstract (6/10), moderately specific (7/10)\n**Feasibility: 6/10, Effectiveness: 8/10**\n\n**Approach B:** Strong on implementation gaps (\u2713), could miss elegant solutions (8/10), very specific (9/10)  \n**Feasibility: 9/10, Effectiveness: 7/10**\n\n**Approach C:** Good on robustness gap (\u2713), could be hard to measure (5/10), needs more specificity (6/10)\n**Feasibility: 5/10, Effectiveness: 8/10**\n\n## 4. SELECT & REFINE\n**Chosen:** Hybrid of A & B - focus on implementation reality while maintaining sophistication measures\n**Top failure mode:** Being too abstract to measure consistently\n**Refinement:** Make each criterion quote-based and adversarially testable\n\n## 5. VERIFY & OUTPUT\n\n# NEW EVALUATION RUBRIC v2.0\n\n## CRITERIA (each 0-3, total 15):\n\n### 1. EVIDENCE DEPTH (0-3)\n- **3:** Multiple specific examples, data, or precedents cited; quantifies claims where possible\n- **2:** Some concrete evidence provided; mix of specific and general support  \n- **1:** Mostly assertions with minimal supporting evidence\n- **0:** Pure opinion/speculation without backing\n\n*Test: \"What specific evidence supports this claim?\"*\n\n### 2. IMPLEMENTATION BARRIERS (0-3)\n- **3:** Identifies specific obstacles (political, resource, technical) with mitigation strategies\n- **2:** Acknowledges some barriers and addresses major ones\n- **1:** Generic awareness of challenges without specific solutions\n- **0:** Ignores or handwaves implementation difficulties\n\n*Test: \"What would prevent this from working and how do you handle that?\"*\n\n### 3. STAKEHOLDER DYNAMICS (0-3)\n- **3:** Maps key players, their incentives, and specific influence strategies\n- **2:** Identifies main stakeholders and considers their perspectives\n- **1:** Generic acknowledgment of \"people involved\"\n- **0:** Treats implementation as purely technical/logical\n\n*Test: \"Who has to say yes and why would they?\"*\n\n### 4. MEASUREMENT PRECISION (0-3)\n- **3:** Defines specific, measurable success metrics with timelines and thresholds\n- **2:** Clear success indicators that could be tracked\n- **1:** Vague directional goals (\"improve,\" \"increase\")\n- **0:** No measurable outcomes specified\n\n*Test: \"How would you know if this worked after 6 months?\"*\n\n### 5. SYSTEMS INTEGRATION (0-3)\n- **3:** Considers ripple effects, unintended consequences, and system-level interactions\n- **2:** Acknowledges broader context and some second-order effects\n- **1:** Limited awareness of wider system impacts\n- **0:** Treats problem in isolation\n\n*Test: \"What else changes when you implement this?\"*\n\n## 6. META\n**What worked:** Hybrid approach captured both sophistication and practicality gaps\n**Do differently:** Could have generated more unconventional approaches initially\n**Transferable insight:** Evaluation systems need regular adversarial testing - when everything scores perfectly, the rubric has become the ceiling rather than the measuring stick",
  "baseline_old": {
    "label": "v1 criteria",
    "scores": [
      0,
      0
    ],
    "avg": 0.0
  },
  "baseline_new": {
    "label": "v2 criteria",
    "scores": [
      0,
      0
    ],
    "avg": 0.0
  },
  "gap_revealed": 0.0
}