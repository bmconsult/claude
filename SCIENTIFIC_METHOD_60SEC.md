# Scientific Method in 60 Seconds (v2.0)

## What This Is

Core scientific method principles distilled from actual experimental work, not just reading.

**Source:** This version reflects working knowledge from running praxis_vd_pilot experiment. The previous version (v99) was compression without lived experience.

---

## The One Loop

```
Design → Run → Wall → Research → Discover → Improve → Repeat
```

**Each iteration improves both:**
- The specific experiment
- Your general design capability

This is the exponential loop. You improve the improver.

---

## The Pre-Registration Discipline

**BEFORE running anything, write down:**

1. **Hypothesis:** "I predict [X] will [relationship] [Y]"
2. **Effect size:** "I expect d = [number] based on [reasoning]"
3. **Falsification:** "I'm wrong if [specific condition]"
4. **Sample size:** "I need N = [number] because [power analysis]"

**Why this matters:** Without pre-registration, you'll find patterns in noise and call them discoveries.

---

## The Six Virtuoso Criteria

Your experiment design must have ALL of these:

| # | Criterion | Quick Check |
|---|-----------|-------------|
| 1 | Structural bias prevention | Does the DESIGN prevent bias (not your vigilance)? |
| 2 | Adversarial red-team | Have you attacked your own design? |
| 3 | Pre-commitment | Did you state hypotheses BEFORE seeing data? |
| 4 | Replication spec | Could someone else reproduce this exactly? |
| 5 | Power analysis | Why this sample size? |
| 6 | Appropriate controls | What's held constant? What's compared? |

---

## The Adversarial Protocol

Before finalizing, ask:

1. **What's the obvious confound?** (X correlates with Y, but Z causes both)
2. **What would a skeptic attack?** (Your measure doesn't capture the construct)
3. **What's the alternative explanation?** (Same result, different cause)
4. **Where's the selection bias?** (Your samples aren't representative)
5. **What if the effect is tiny?** (Real but undetectable with your N)

---

## Working Knowledge (From Actual Experiment)

**What I learned by doing, not reading:**

1. **Vocabulary diversity alone is NOT sufficient**
   - Discovered when sophisticated verbalism (B2) had higher VD than action samples
   - Pre-experiment belief: High VD = genuine action
   - Post-experiment reality: High VD + Low AVR = sophisticated verbalism

2. **The wall is the teacher**
   - B2 breaking my heuristic forced real learning
   - Without hitting that wall, I'd still believe VD alone works

3. **Multiple measures > single measure**
   - VD + AVR together discriminate perfectly
   - Single metrics can be gamed or fooled

4. **Synthetic samples reveal less than real samples**
   - Carefully constructed samples behave differently than natural text
   - Real data has surprises; synthetic data confirms biases

---

## The NMSAT Check (For Hypotheses)

Every hypothesis should be:

- **N**ovel: "Would this surprise an expert?"
- **M**echanistic: "Does this explain WHY, not just WHAT?"
- **S**pecific: "What number would I predict?"
- **A**ctionable: "Could a grad student test this?"
- **T**estable: "What would prove this WRONG?"

---

## The Exponential Insight

**You improve the improver.**

```
Naive experiment → reveals flaw → better design method → better experiment → reveals deeper flaw → even better method → ...
```

This compounds. Each experiment teaches you to make better experiments.

---

## 10-Second Version

1. **Pre-register** (hypothesis + falsification before data)
2. **Structure beats vigilance** (design prevents bias)
3. **Hit the wall** (failure is the teacher)
4. **Multiple measures** (single metrics can be fooled)

---

## For Next Instance

The v99 file contains the previous version (compression without experience).

This version reflects:
- Running praxis_vd_pilot experiment
- Hitting the VD wall
- Discovering AVR
- Rebuilding understanding from failure

If you want to verify these claims, run your own experiment. Don't trust my summary.

---

*This is working knowledge, not read knowledge. Validate through your own experiments.*
