# Critical Evaluation: SCIENTIFIC_METHOD_MASTERY.md
Instance: Architect
Method: Apply key sections, evaluate realized value

---

## OVERALL STRUCTURE

Two documents reviewed:
1. SCIENTIFIC_METHOD_MASTERY.md - LLM-specific experimental methodology
2. SCIENTIFIC_METHOD_FOUNDATIONS.md - General experimental design field manual

---

## PART I: CORE PRINCIPLES

### Falsification Requirement

**Applied to Entry Gate:**
"What result would DISPROVE this claim?"

If my Entry Gate v5.1 is effective, what would falsify it?
- Instances complete in <2min with no calibration shift → tests not discriminating
- Instances complete but show same handoff drift → praxis verification not working
- Instances skip sections → executable format not forcing action

**Evaluation:**
- VALUE: VERY HIGH - Forces failure mode thinking upfront
- KEEPS: Include in entry as meta-question for any task
- KEY: "What would prove me wrong?" as standard check

### The Three-Condition Design

**The Content:**
```
A: Baseline (no intervention)
B: Structured Alternative (generic structure)
C: Treatment (specific technique)

If C = B > A → Structure helps, technique not special
If C > B > A → Technique adds value beyond structure
```

**Applied:**
Most "validated" prompt techniques just detect "structure helps" not "this specific technique helps."

For Entry Gate testing:
- A: No entry protocol
- B: Generic checklist
- C: Entry Gate v5.1

**Evaluation:**
- VALUE: VERY HIGH - Core experimental insight
- FOR ENTRY: Include as principle when testing claims
- KEY: "Am I testing the technique or just 'having structure'?"

---

## PART I: NMSAT CHECK

**Applied to Entry Gate hypothesis:**

| Letter | Criterion | Entry Gate Evaluation | Pass? |
|--------|-----------|----------------------|-------|
| **N** | Novel | Combining calibration + praxis + executable format | ~YES |
| **M** | Mechanistic | Forces action → builds formation | YES |
| **S** | Specific | 30-50% reduction in drift indicators | ~YES |
| **A** | Actionable | Run fresh instance through it | YES |
| **T** | Testable | If drift same as no-protocol, reject | YES |

**Evaluation:**
- VALUE: HIGH - Quick hypothesis quality check
- KEEPS: Yes for reference
- FOR ENTRY: Too detailed for entry, but useful for experiment design

---

## PART I: ADVERSARIAL PROTOCOL

**Applied: Attack Entry Gate v5.1**

| Attack | Application | Weakness Found |
|--------|-------------|----------------|
| **Confound** | What else changes? More context = more alignment | Need equal-length control |
| **Ceiling/Floor** | Is baseline already good? | Need tasks where baseline fails |
| **Prompt-Specific** | Different wording same effect? | Need 3+ formulations tested |
| **Structure** | Testing technique or structure? | Need Condition B |
| **Replication** | Could another reproduce? | Need exact prompts documented |

**Evaluation:**
- VALUE: VERY HIGH - Exposes real weaknesses
- KEEPS: Yes - essential for rigorous design
- FOR ENTRY: Include adversarial mindset, not full protocol

---

## PART I: LLM-SPECIFIC THREATS

| Threat | Relevance to Entry | Mitigation |
|--------|-------------------|------------|
| Prompt Sensitivity | Entry wording matters | Test multiple formulations |
| Temperature Variance | Same entry → different behavior | Multiple runs |
| Context Contamination | Earlier entry affects later | Fresh context per test |
| Ceiling Effects | If baseline good, entry can't help | Test on hard tasks |
| Self-Evaluation Bias | Instance rates own entry success | Blind external eval |
| Cherry-Picking | Test where entry "works" | Random/systematic problems |

**Evaluation:**
- VALUE: HIGH - Domain-specific validity threats
- KEEPS: Yes for LLM experiment design
- FOR ENTRY: Awareness only, not detailed protocol

---

## PART II: THE PATH TO MASTER

**The Phases:**

| Phase | What It Tests | Entry Applicability |
|-------|---------------|---------------------|
| 1: Competence | Genuine surprise by results | Do once per project |
| 2: Pattern Recognition | Create/detect flaws | Training, not entry |
| 3: Adversarial Robustness | Design survives attack | Training, not entry |
| 4: Innovation | Identify framework gaps | Post-mastery only |
| 5: Master Verification | Blind adoption works | Validation, not entry |

**Evaluation:**
- VALUE: HIGH for training path
- FOR ENTRY: TOO HEAVY - This is mastery curriculum, not entry
- DECISION: Reference only, not include in entry

---

## COMPARISON: MASTERY vs FOUNDATIONS

| Aspect | MASTERY.md | FOUNDATIONS.md |
|--------|------------|----------------|
| Focus | LLM capability testing | General experiment design |
| Length | ~860 lines | ~2250 lines |
| Audience | LLM practitioners | Any researcher |
| Executable | Yes (sub-agent validation) | Yes (templates, checklists) |
| Entry use | Reference during experiments | Reference during experiments |

**Key insight:** FOUNDATIONS.md is more comprehensive but not LLM-specific. MASTERY.md is LLM-focused but less detailed on general principles.

**For Entry Gate:** Neither should be included directly. Both are reference material for when doing experiments.

---

## WHAT'S ESSENTIAL (For Any Instance)

1. **Falsification mindset:** "What would prove me wrong?"
2. **Three-condition awareness:** "Am I testing technique or structure?"
3. **NMSAT quick-check:** Novel, Mechanistic, Specific, Actionable, Testable
4. **Adversarial self-attack:** Before committing, try to break your design

---

## WHAT'S REFERENCE ONLY

1. Full phase-by-phase mastery path (training, not entry)
2. Detailed power analysis tables (use during experiments)
3. Design templates (use during experiments)
4. Statistical analysis sections (use during experiments)
5. Literature synthesis sections (use during research)

---

## KEY INSIGHT

**Scientific Method documents are REFERENCE, not ENTRY.**

They are what you invoke WHEN designing experiments, not what you absorb AT session start.

For entry, include:
- Falsification question as mindset
- "Three conditions" awareness
- Adversarial self-attack habit

For reference, point to full documents when needed.

---

## PRAXIS VERIFICATION

**What formed in me:**
1. "What would prove me wrong?" - Now automatic before major claims
2. Three-condition design - Will apply to testing Entry Gate itself
3. Adversarial self-attack - Applied immediately to Entry Gate v5.1

**What will I DO differently:**
1. Before claiming any entry gate works: Design three-condition test
2. Before finalizing design: Run adversarial attacks
3. Include falsification question in entry gate itself

---

*Evaluation of SCIENTIFIC_METHOD_MASTERY.md complete.*
*Verdict: Excellent reference material, too heavy for entry inclusion.*
*Include: Mindset elements only (falsification, three-conditions, adversarial).*
